<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[SparkSQL执行全过程概述]]></title>
    <url>%2F2021%2F01%2F10%2F2021-01-10-SparkSQL%E6%89%A7%E8%A1%8C%E5%85%A8%E8%BF%87%E7%A8%8B%E6%A6%82%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[Preface本文为阅读 “Spark SQL内核剖析” 第三章 “Spark SQL 执行全过程概述”所做的归纳和整理。以下内容主要基于Spark 2.1 &amp; Spark 2.2。首先，作者在该章的第一部分介绍了Spark SQL 执行优化器Catalyst涉及的重要概念，包括InternalRow, TreeNode体系和Expression体系。在该章的第二部分，作者介绍了从一段SQL语句到Spark可以执行的RDD[InternalRow]类型的转换所要经历的三个阶段, 逻辑计划， 物理计划以及代码的生产以及提交。 Catalyst涉及的重要概念本节先简要介绍C a t al y st 中涉及的重要概念和数据结构,主要包括I n t e r n a l R o w体系、T r e e N o d e体系和E x p r e s si o n 体系 InternalRowI n t e r n a l R o w 就是用来表示一行行数据的类,因此图3. 3 中物理算子树节点产生和转换的R D D类型即为R D D[ I n t e r n a l R o w] 。 InternalRow体系 B a s e G e n e r i c l n t e r n a l R o w : 同样是一个抽象类,实现了I n t e r n a l R o w 中定义的所有g e t 类型方法,这些方法的实现都通过调用类中定义的g e n e r i c G e t 虚函数进行,该函数的实现在下一级子类中 1.G e n e r i c l n t e r n a l ­ R o w GenericInternalRow 不可变 ，G e n e r i c l n t e r n a l R o w 构造参数是A r r a y [ A n y ]类型,采用对象数组进行底层存储,g e n e r i c G e t 也是直接根据下标访问的。 这里需要注意,数组是非拷贝的,因此一旦创建,就不允许通过s e t 操作进行改变。 2.S p e c i f i c i n t e r n a l R o w 可变 ，S p e c i f i c l r 出r n a l R o w 则是以A r r a y [ M u t a b l e V a l u e ] 为构造参数的, 允许通过s e t 操作进行修改 3.M u t a b l e U n s a f e R o w Joined Row 顾名思义,该类主要用于J o i n 操作,将两个I n t e r n a l R o w 放在一起形成新的I n t e r n a l R o w。使用时需要注意构造参数的顺序。 Unsafe Row 不采用J a v a 对象存储的方式,避免了J V M中垃圾回收(G C)的代价。此外,U n s a f e R o w 对行数据进行了特定的编码,使得存储更加高效。 Tree Node体系T r e e N o d e 类是S p a r k S Q L中所有树结构的基类,定义了一系列通用的集合操作和树遍历操作接口。 T r e e N o d e 内部包含一个S e q [ B a s e T y p e ] 类型的变量c h i l d r e n 来表示孩子节点。 TreeNode体系 两个子类继承体系T r e e N o d e 提供的仅仅是一种泛型,实际上包含了两个子类继承体系, Q u e r y Plan E x p r e s s i o n 体系 基本操作TreeNode basic operation TreeNode基本操作 C a t a l y s t 中还提供了节点位置功能,即能够根据T r e e N o d e 定位到对应的S Q L 字符串中的行数和起始位置。 Expression表达式一般指的是不需要触发执行引擎而能够直接进行计算的单元,例如加减乘除四则运算、逻辑操作、转换操作、过滤操作等。 在E x p r e s s i o n 类中,主要定义了5 个方面的操作 Expression基本操作 基本属性Foldable 该属性用来标记表达式能否在查询执行之前直接静态计算。目前,f o l d a b l e 为t r u e 的情况有两种 Literal第一种是该表达式为L i t e r a l 类型(“字面值”,例如常量等) Child Expression Foldable第二种是当且仅当其子表达式中f o l d a b l e 都为t r u e 时 Deterministic 协助算子优化（谓词下推）该属性用来标记表达式是否为确定性的,即每次执行e v a l 函数的输出是否都相同。考虑到S p a r k 分布式执行环境中数据的S h u f f l e 操作带来的不确定性,以及某些表达式(如R a n d 等)本身具有不确定性,该属性对于算子树优化中判断谓词能否下推等很有必要。 r e f e r e n c e s canonicalized 规范化处理会在确保输出结果相同的前提下通过一些规则对表达式进行重写 SemanticEqual 判断两个表达式在语义上是否等价。 两个表达式都是确定性的( d e t e r m i n i s t i c 为t r u e ) 两个表达式经过规范化处理后(C a n o n i c a l i z e d )仍然相同。 核心操作输入输出字符串表达式等价性判断从SQL 到 RDD一个简单的例子123val spark = SparkSession.builder().appName("example").master("local").getOrCreate()spark.read.json("student.json").createOrReplaceTemplateView("student")spark.sql("select name from student where age&gt;18").show() 上述代码主要做了如下几件事情 （1）创建SparkSession, spark程序的入口 （2）读取数据（数据源可以为本地文件, hdfs 或者 hive等） （3）执行spark sql语句并将结果收集到driver端 Logic Plan逻辑计划阶段会将用户所写的S Q L 语句转换成树型数据结构(逻辑算子树),S Q L 语句中蕴含的逻辑映射到逻辑算子树的不同节点 SQL执行全过程概览 实际转换过程 逻辑算子树的生成过程经历3 个子阶段 Unresolved Logic PlanU n r e s o l v e d L o g i c a ! P l a n Analyzed Logic PlanA n a l y z e d L o g i c a l P l a n Optimized Logic PlanO p t i m i z e dlo g i c a l P l a n Physical Plan物理计划阶段将上一步逻辑计划阶段生成的逻辑算子树进行进一步转换,生成物理算子树。物理算子树的节点会直接生成R D D或对R D D进行t r a n s f o r m a t i o n 操作(注:每个物理计划节点中都实现了对R D D进行转换的e x e c u t e 方法) Interator[physical plan]生成物理算子树的列表I t e r a t o r [ P h y s i c a l P l a n ] Spark Plan从列表中按照一定的策略选取最优的物理算子树(S p a r k P l a n ) Prepared Spark Plan对选取的物理算子树进行提交前的准备工作,例如,确保分区操作正确、物理算子树节点重用、执行代码生成等,得到“准备后”的物理算子树(P r e p a r e dS p a r k P l a n) Submit物理算子树生成的RDD执行a c t i o n 操作 Driver端运行从S Q L 语句的解析一直到提交之前,上述整个转换过程都在S p a r k 集群的D r i v e r 端进行,不涉及分布式环境。 Physical Plan 根结点 ProjectExec生成的物理算子树根节点是P r o j e c t E x e c ,每个物理节点中的e x e c u t e 函数都是执行调用接口,由根节点开始递归调用,从叶子节点开始执行。 F i l e S o u r c e S c a n E x e c 叶子执行节点中需要构造数据源对应的R D D , Filter E x e c 和P r o j e c t E x e c 中的e x e c u t e 函数对R D D 执行相应的t r a n s f o r m a t i o n 操作。 Spark S Q L内部实现上述流程中平台无关部分的基础框架称为C a t a l y s t 内部数据类型数据类型主要用来表示数据表中存储的列信息,常见的数据类型包括简单的整数、浮点数、字符串,以及复杂的嵌套结构等。]]></content>
      <categories>
        <category>大数据开发</category>
      </categories>
      <tags>
        <tag>spark sql</tag>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark聚合操作]]></title>
    <url>%2F2021%2F01%2F09%2F2021-01-09-Spark%E8%81%9A%E5%90%88%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[前言本文为阅读”Spark The Definitive Guide”Chapter 7 所做的归纳和整理，部分代码来自于书中，部分为自己在本机试验所得Definition of AggregationAggregation is the act of collecting something together. Brief Overview聚合的类型主要有以下几种 针对整个数据集(Dataframe) 分组聚合 窗口聚合 Grouping Sets rollup （Hierarchically） cube （所有可能的组合） 返回的结果: RelationalGroupedDataset 1234567val df = spark.read.format("csv") .option("header", "true") .option("inferSchema", "true") .load("/data/retail-data/all/*.csv") .coalesce(5)df.cache()df.createOrReplaceTempView("dfTable") Aggregation Functions聚合函数大部分被定义在 or.apache.spark.sql.functions count12import org.apache.spark.sql.functions.countdf.select(count("StockCode")).show() // 541909 countDistinct12import org.apache.spark.sql.functions.countDistinctdf.select(countDistinct("StockCode")).show() // 4070 approx_count_distinct12import org.apache.spark.sql.functions.approx_count_distinctdf.select(approx_count_distinct("StockCode", 0.1)).show() // 3364 first and last12import org.apache.spark.sql.functions.&#123;first, last&#125;df.select(first("StockCode"), last("StockCode")).show() min and max12import org.apache.spark.sql.functions.&#123;min, max&#125;df.select(min("Quantity"), max("Quantity")).show() sum12import org.apache.spark.sql.functions.sumdf.select(sum("Quantity")).show() // 5176450 sumDistinct12import org.apache.spark.sql.functions.sumDistinctdf.select(sumDistinct("Quantity")).show() // 29310 avg1234567891011import org.apache.spark.sql.functions.&#123;sum, count, avg, expr&#125;df.select( count("Quantity").alias("total_transactions"), sum("Quantity").alias("total_purchases"), avg("Quantity").alias("avg_purchases"), expr("mean(Quantity)").alias("mean_purchases")) .selectExpr( "total_purchases/total_transactions", "avg_purchases", "mean_purchases").show() variance and standard deviation1234import org.apache.spark.sql.functions.&#123;var_pop, stddev_pop&#125;import org.apache.spark.sql.functions.&#123;var_samp, stddev_samp&#125;df.select(var_pop("Quantity"), var_samp("Quantity"), stddev_pop("Quantity"), stddev_samp("Quantity")).show() skewness and kurtosis12import org.apache.spark.sql.functions.&#123;skewness, kurtosis&#125;df.select(skewness("Quantity"), kurtosis("Quantity")).show() covariance and correlation123import org.apache.spark.sql.functions.&#123;corr, covar_pop, covar_samp&#125;df.select(corr("InvoiceNo", "Quantity"), covar_samp("InvoiceNo", "Quantity"), covar_pop("InvoiceNo", "Quantity")).show() aggregating to complex types12import org.apache.spark.sql.functions.&#123;collect_set, collect_list&#125;df.agg(collect_set("Country"), collect_list("Country")).show() Grouping分组聚合,输入多行，输出一行 Grouping With Expressions123df.groupBy("InvoiceNo").agg( count("Quantity").alias("quan"), expr("count(Quantity)")).show() Grouping With Maps1df.groupBy("InvoiceNo").agg("Quantity"-&gt;"avg", "Quantity"-&gt;"stddev_pop").show() Window Functions对输入的每一行在一个分组的窗口范围内做计算，最后为每一行输出一个结果 执行一个窗口函数需要两个要素 定义一个窗口 定义一个窗口函数 , 输入: 窗口内的所有行， 输出：一行，目前Spark支持三种窗口函数 aggregate function ranking function analytic function 如何定义一个窗口一个窗口的定义需要三个要件 分组定义，即partition by 的条件，窗口只在指定的分组内生效 分组内排序条件，即order by的条件，指定了在一个分组内的每一行如何排列 Window Frame的定义，即rowsBetween, 指定了哪些行可以落在当前行的窗口内 为了更好的说明，举个例子 123456import org.apache.spark.sql.expressions.Windowimport org.apache.spark.sql.functions.colval windowSpec = Window .partitionBy("CustomerId", "date") .orderBy(col("Quantity").desc) .rowsBetween(Window.unboundedPreceding, Window.currentRow) 上述代码定义了一个窗口，首先Window.partitionBy(&quot;CustomerId&quot;, &quot;date&quot;) 指定了当前窗口的分组为相同CustomerId, 相同日期的行，接着orderBy指定了在分组内按照Quantity来排序，最后，rowsBetween 定义了对于每一行来说窗口作用的范围，在上述例子中代表当前行前的所有行到当前行，当然也可以用数字，例如rowsBetween(-1,0)代表前一行到当前行 定义一个窗口函数目前Spark支持的窗口函数主要有三种 aggregate function ranking function analytic function 分别举例说明 aggregate function12import org.apache.spark.sql.functions.maxval maxPurchaseQuantity = max(col("Quantity")).over(windowSpec) 上述代码返回了窗口内的最大Quantity值 ranking function123import org.apache.spark.sql.functions.&#123;dense_rank, rank&#125;val purchaseDenseRank = dense_rank().over(windowSpec)val purchaseRank = rank().over(windowSpec) dense_rank() 返回去重后的rank rank() 返回真实排名，如果同一个排名有多个相同的值，后续的rank依次累加 完整的例子Scala1234567891011121314151617181920212223242526272829303132import org.apache.spark.sql.functions.&#123;col, to_date&#125;val dfWithDate = df.withColumn("date", to_date(col("InvoiceDate"), "MM/d/yyyy H:mm"))dfWithDate.createOrReplaceTempView("dfWithDate")// in Scalaimport org.apache.spark.sql.expressions.Windowimport org.apache.spark.sql.functions.colval windowSpec = Window .partitionBy("CustomerId", "date") .orderBy(col("Quantity").desc) .rowsBetween(Window.unboundedPreceding, Window.currentRow)import org.apache.spark.sql.functions.maxval maxPurchaseQuantity = max(col("Quantity")).over(windowSpec)// in Scalaimport org.apache.spark.sql.functions.&#123;dense_rank, rank&#125;val purchaseDenseRank = dense_rank().over(windowSpec)val purchaseRank = rank().over(windowSpec)// in Scalaimport org.apache.spark.sql.functions.coldfWithDate.where("CustomerId IS NOT NULL").orderBy("CustomerId") .select( col("CustomerId"), col("date"), col("Quantity"), purchaseRank.alias("quantityRank"), purchaseDenseRank.alias("quantityDenseRank"), maxPurchaseQuantity.alias("maxPurchaseQuantity")).show() SQL12345678910111213141516171819SELECT CustomerId, date, Quantity, rank(Quantity) OVER (PARTITION BY CustomerId, date ORDER BY Quantity DESC NULLS LAST ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) as rank, dense_rank(Quantity) OVER (PARTITION BY CustomerId, date ORDER BY Quantity DESC NULLS LAST ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) as dRank, max(Quantity) OVER (PARTITION BY CustomerId, date ORDER BY Quantity DESC NULLS LAST ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) as maxPurchaseFROM dfWithDate WHERE CustomerId IS NOT NULL ORDER BY CustomerId Grouping Setsgrouping sets可以理解为按照一个或多个维度分组并统计，grouping sets只作用于SQL 例如，下面两段SQL在结果上是等效的 123SELECT CustomerId, stockCode, sum(Quantity) FROM dfNoNullGROUP BY customerId, stockCodeORDER BY CustomerId DESC, stockCode DESC 123SELECT CustomerId, stockCode, sum(Quantity) FROM dfNoNullGROUP BY customerId, stockCode GROUPING SETS((customerId, stockCode))ORDER BY CustomerId DESC, stockCode DESC 如果我们想要得到对整个结果集以及根据 (CustomerId, stockCode)维度的统计，则需要用到grouping sets 123SELECT CustomerId, stockCode, sum(Quantity) FROM dfNoNullGROUP BY customerId, stockCode GROUPING SETS((customerId, stockCode),())ORDER BY CustomerId DESC, stockCode DESC 同时需要注意的是,grouping sets的数据源必须没有null,否则将会和结果集的null有冲突 RollupsRollups 将 group中的每个元素按照层级进行group by，例如 123456789val rolledUpDf = Seq( ("2020-01-01", "USA", "30"), ("2020-01-02", "USA", "70"), ("2020-01-01","China","90")) .toDF("Date","Country","Quantity") .rollup("Date","Country") .agg(sum("Quantity")) .selectExpr("Date", "Country","`sum(Quantity)` as totalQuantity") rolledUpDf.show(false) 上述代码将按照三个level进行聚合 （） group by (Date) group by (Date, Country) Cube按照所有可能的组合进行group by 123456789val cubedDF = Seq( ("2020-01-01", "USA", "30"), ("2020-01-02", "USA", "70"), ("2020-01-01","China","90")) .toDF("Date","Country","Quantity") .cube("Date","Country") .agg(sum("Quantity")) .selectExpr("Date", "Country","`sum(Quantity)` as totalQuantity") cubedDF.show(false) Grouping Metadata为不同的grouping打上编号 1234567891011import org.apache.spark.sql.functions.&#123;grouping_id, sum, expr&#125;val cubedDF = Seq( ("2020-01-01", "USA", "30"), ("2020-01-02", "USA", "70"), ("2020-01-01","China","90")) .toDF("Date","Country","Quantity") .cube("Date","Country") .agg(grouping_id(),sum("Quantity")) .orderBy(expr("grouping_id()").desc) cubedDF.show(false) Pivot行转列 12val pivoted = dfWithDate.groupBy("date").pivot("Country").sum()pivoted.where("date &gt; '2011-12-05'").select("date" ,"`USA_sum(Quantity)`").show() User-Defined Aggregation Functions用户自定义聚合函数（UDAF）的几个要件 inputSchema bufferSchema deterministic initialize update merge evaluate 例子：实现一个聚合函数，输入多行，返回多行是否都为true 12345678910111213141516171819202122232425import org.apache.spark.sql.expressions.MutableAggregationBufferimport org.apache.spark.sql.expressions.UserDefinedAggregateFunctionimport org.apache.spark.sql.Rowimport org.apache.spark.sql.types._class BoolAnd extends UserDefinedAggregateFunction &#123; def inputSchema: org.apache.spark.sql.types.StructType = StructType(StructField("value", BooleanType) :: Nil) def bufferSchema: StructType = StructType( StructField("result", BooleanType) :: Nil ) def dataType: DataType = BooleanType def deterministic: Boolean = true def initialize(buffer: MutableAggregationBuffer): Unit = &#123; buffer(0) = true &#125; def update(buffer: MutableAggregationBuffer, input: Row): Unit = &#123; buffer(0) = buffer.getAs[Boolean](0) &amp;&amp; input.getAs[Boolean](0) &#125; def merge(buffer1: MutableAggregationBuffer, buffer2: Row): Unit = &#123; buffer1(0) = buffer1.getAs[Boolean](0) &amp;&amp; buffer2.getAs[Boolean](0) &#125; def evaluate(buffer: Row): Any = &#123; buffer(0) &#125;&#125; 12345678910111213object BoolAnd &#123; def main(args: Array[String]):Unit = &#123; val ba = new BoolAnd val spark = SparkSession.builder().master("local").appName("yifan_spark_test2") .getOrCreate() spark.udf.register("booland", ba) spark.range(1) .selectExpr("explode(array(TRUE, TRUE, TRUE)) as t") .selectExpr("explode(array(TRUE, FALSE, TRUE)) as f", "t") .select(ba(col("t")), expr("booland(f)")) .show() &#125;&#125; Q&amp;ASpark SQL 在执行聚合时（groupBy）底层调用了哪些函数？(TODO)countDistinct 和 groupBy哪一个性能比较好? (TODO)]]></content>
      <categories>
        <category>大数据开发</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>aggregation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Introduction Of Spark Cache]]></title>
    <url>%2F2020%2F11%2F28%2F2020-11-28-Introduction%20Of%20Spark%20Cache%2F</url>
    <content type="text"><![CDATA[Introduction Of Spark CachePrefaceIn Spark Sql, the use of cache is common when you need to reuse some intermediate computation result. Understanding the mechanism of spark cache can help developers speed up the computation process and raise the efficiency. This blog will help readers build up a whole picture of spark cache by answering three key questions. First I will briefly introduce what spark cache is by giving specific coding examples in scala. Then I will illustrate the scenario when developers may use spark cache. Finally I will illustrate the working mechanism of spark cache and some notation points derived from my own experience. What is Spark Cache?Spark Cache offers a way such that you can store your computation result in memory or disk. In Scala1.cache() 123val cachedDf = dataframe.select("col1", "col2").filter("age &gt; 10").cache()cachedDf.filter("sex is female").count()// This is when the caching process is triggeredcachedDf.filter("hometown is China").show()//This is when the cached df in memory is used 2.persisit() &amp; unpersist() 123456789val cachedDf = dataframe.select("col1", "col2").filter("age &gt; 10").persist(StorageLevel.MEMORY_AND_DISK)cachedDf.filter("sex is female").count()// This is when the caching process is triggeredcachedDf.filter("hometown is China").show()//This is when the cached df in memory is usedcachedDf.unpersist()// After which cachedDf is no longer used in subsequent action operations In SQL123spark.sql("cache table table_name")spark.sql("cache lazy table table_name")spark.sql("uncache table table_name") Types of Spark CacheThere are two ways that you can achieve caching in spark. One way is to use cache() method for which the only storage level is MEMORY_ONLY. The other way is to use persist() with optional storage level . Cache (Source Code in Scala)1234/** * Persist this RDD with the default storage level (`MEMORY_ONLY`). */ def cache(): this.type = persist() Persist DISK_ONLY: Persist data on disk only in serialized format. MEMORY_ONLY: Persist data in memory only in deserialized format. MEMORY_AND_DISK: Persist data in memory and if enough memory is not available evicted blocks will be stored on disk. OFF_HEAP: Data is persisted in off-heap memory. Refer spark.memory.offHeap.enabled in Spark Doc. More Storage Level in StorageLevel.scala When to use Spark Cache?The rule of thumb for caching is to identify the Dataframe that you will be reusing in your Spark Application and cache it. In other words, use Spark Cache when the target Dataframe(intermediate result) is used by two or more action operations. Benefit Raise the efficiency of I&amp;O since reading data directly from hdfs file is time consuming while reading directly from the executor’s memory or disk is fast and stable. Create a check point where spark can recompute the lost RDD directly from the cached Dataframe How Spark Cache Works? When you call cache on a Dataframe, nothing happens with the data but the query plan is updated by adding a new operator—InMemoryRelation Spark use Cache Manager to keep track of what computation has already been cached in terms of the query plan. The phase of the Cache Manager happend before the optimizer, after the analyzer in the logic plan stage. When you call an action operation on the Dataframe, it will check if some subquery has already been flaged as “cached” by comparing the analyzed logical plan. The Caching Process is triggered only when the first action operation is called on some dataframe that use this cachedDF in subquery. Caution In Usage Always use persist with storage level MEMORY_AND_DISK instead of cache(). You should be cautious when you call cache() method on a Datasets or Dataframe. Because the storage level is MEMORY_ONLY, if you attempt to cache a very large Datase, it may trigger the OOM(Out Of Memory) Exception. Unpersist the cachedDf when you no longer need it in subsequent computation to release in-memory space. There are situations where caching doesn’t help at all and on the contrary slows down the execution. This is related for instance to queries based on large datasets stored in a columnar file format that supports column pruning and predicate pushdown such as parquet. (See https://towardsdatascience.com/best-practices-for-caching-in-spark-sql-b22fb0f02d34) Further Thoughts Difference between cache and checkpoint How Spark manage memory use Reference List RDD.scala apache-spark-caching best-practices-for-caching-in-spark-sql https://spark.apache.org/docs/latest/configuration.html#memory-management http://spark.apache.org/docs/latest/api/scala/org/apache/spark/rdd/RDD.html]]></content>
      <categories>
        <category>大数据开发</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>Scala</tag>
        <tag>Spark Cache</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark Dataframe基本操作]]></title>
    <url>%2F2020%2F08%2F23%2F2020-08-23-Spark%20Dataframe%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[Spark Dataframe基本操作创建DataFrame12val df=spark.read.format("json").load("/data/flight-data/json/2015-summary.json")df.createOrReplaceTempView("dfTable")123456789val myManualSchema = new StructType(Array(new StructField("some",StringType,true), new StructField("col",StringType,true), new StructField("names",LongType,false) )) val myRows=Seq(Row("Hello",null,1L))val myRDD=spark.sparkContext.parallelize(myRows)val myDf=spark.createDataFrame(myRDD,myManualSchema) 1val myDF=Seq("Hello",2,1L).toDF("col1","col2","col3") Select1df.select("col1","col2").show(2) 12345678df.select( df.col("column1"), col("column1"), column("column1"), 'column1', $"column1", expr("column1")).show(2) 1234df.selectExpr("*","DEST_COUNTRT_NAME=ORIGIN_COUNTRY_NAME" as WithinCountry).show(2) 字面量1df.select(expr("*"),lit(1).as("One")).show(2) 添加列1df.withColumn("numberOne",lit(1)).show(2) 1df.withColumn("withinCountry",expr("col1==col2")).show(2) 重命名列1df.withColumnRenamed("old","new").columns 删除列123df.drop("col1").columnsdf.withColumn("count2",col("count").cast("long")) 过滤行1234df.filter(col("count")&lt;2).show(2)df.where("count &lt; 2").show(2)df.where("").where("") 行排序12df.orderBy("count","DEST_COUNTRY_NAME").show()df.orderBy(desc("count"),asc("col2")).show(2)]]></content>
      <categories>
        <category>大数据开发</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop各组件基本介绍]]></title>
    <url>%2F2020%2F08%2F23%2F2020-08-23-Hadoop%E5%90%84%E7%BB%84%E4%BB%B6%E5%9F%BA%E6%9C%AC%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[Hadoop各组件基本介绍前言Hadoop作为大数据生态中很重要的一环，其各个组件构成了搭建大数据平台的基础。了解清楚其底层各组件，有助于更好的进行相关的开发大纲 HDFS MapReduce Hive Hbase HDFSHadoop Distributed File System实现了一个分布式文件存储系统，简单的来说就是将海量的文件分布的存储在不同的物理磁盘上，使用者不需要关心具体某部分数据被存储在哪里，而能够像在单机文件系统上一样存储读取文件。 HDFS的架构 NameNode: 维护文件系统数和整颗树上的各个节点 DataNode: 数据真正存放的位置 MapReduce引用 分布式计算的框架 Map: 将任务拆封成若干个子任务，按照一定的规则进行匹配计算,生成一定的Key-Value Reduce: 将相同key的value进行迭代计算 HiveHive是一个将sql语句翻译成map reduce命令的解释器，分为两部分 存储hive表元数据的mysql hdfs:数据真正存储的地方 Hbase列簇数据库，底层依然是HDFS,Hbase不同于传统的行数据库，Hbase可以随时增加列。一个行键、列族、列修饰符、数据和时间戳组合起来叫做一个单元格（Cell）。这里的行键、列族、列修饰符和时间戳其实可以看作是定位属性（类似坐标），最终确定了一个数据。Hbase适用大数据量的实时搜索。 轻松理解Hbase面向列的存储]]></content>
      <categories>
        <category>大数据开发</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>hdfs</tag>
        <tag>mapreduce</tag>
        <tag>hive</tag>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop各组件基本介绍]]></title>
    <url>%2F2020%2F08%2F23%2F2020-08-23-Spark%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%9E%B6%E6%9E%84%2F</url>
    <content type="text"><![CDATA[Spark的基本架构Spark应用程序组成 一个Driver 运行main()函数 维护spark应用程序的相关信息 回应用户的程序或输入 分析任务并分发给若干executor进行处理 一组Executor 执行driver分发给它的代码 将该执行器的计算状态报告给运行driver的节点 重要概念 SparkSession是代码的入口点 分区是位于集群中的一台物理机上的多行数据的集合 转换操作 1val divisBy2 = myRange.where("number % 2 =0") 这些转换并没有实际输出，在调用动作操作之前，Spark不会真的执行转换操作，有两类转换操作 窄依赖 宽依赖 Lazy Evaluation: 等到绝对需要时才执行计算 动作操作：触发计算的Action]]></content>
      <categories>
        <category>大数据开发</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>hdfs</tag>
        <tag>mapreduce</tag>
        <tag>hive</tag>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka基础]]></title>
    <url>%2F2020%2F08%2F23%2F2020-08-23-Kafka%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[Kafka基础基本介绍“官网”Technically speaking, event streaming is the practice of capturing data in real-time from event sources like databases, sensors, mobile devices, cloud services, and software applications in the form of streams of events; storing these event streams durably for later retrieval; manipulating, processing, and reacting to the event streams in real-time as well as retrospectively; and routing the event streams to different destination technologies as needed. To publish (write) and subscribe to (read) streams of events, including continuous import/export of your data from other systems. To store streams of events durably and reliably for as long as you want. To process streams of events as they occur or retrospectively. 工作原理分布式系统根据TCP协议通信 ServersKafka is run as a cluster of one or more servers that can span multiple datacenters or cloud regions. Some of these servers form the storage layer, called the brokers. Other servers run Kafka Connect to continuously import and export data as event streams to integrate Kafka with your existing systems such as relational databases as well as other Kafka clusters. ClientsThey allow you to write distributed applications and microservices that read, write, and process streams of events in parallel, at scale, and in a fault-tolerant manner even in the case of network problems or machine failures. 核心概念 In Kafka, producers and consumers are fully decoupled and agnostic of each other, which is a key design element to achieve the high scalability that Kafka is known for. Event(事件) Producers(生产者) Consumers(消费者) EventEvents are organized and durably stored in topics. Very simplified, a topic is similar to a folder in a filesystem, and the events are the files in that folder. An example topic name could be “payments”. Topics in Kafka are always multi-producer and multi-subscriber: a topic can have zero, one, or many producers that write events to it, as well as zero, one, or many consumers that subscribe to these events. Events in a topic can be read as often as needed—unlike traditional messaging systems, events are not deleted after consumption. key value timestamp metadata header ExampleEvent key: “Alice”Event value: “Made a payment of $200 to Bob”Event timestamp: “Jun. 25, 2020 at 2:06 p.m.” ProducersProducers are those client applications that publish (write) events to KafkaConsumersconsumers are those that subscribe to (read and process) these events Topics Topics are partitioned, meaning a topic is spread over a number of “buckets” located on different Kafka brokers. Kafka guarantees that any consumer of a given topic-partition will always read that partition’s events in exactly the same order as they were written A common production setting is a replication factor of 3, i.e., there will always be three copies of your data. This replication is performed at the level of topic-partitions. Kafka设计All data is immediately written to a persistent log on the filesystem without necessarily flushing to disk. In effect this just means that it is transferred into the kernel’s pagecache. Intuitively a persistent queue could be built on simple reads and appends to files as is commonly the case with logging solutions. This structure has the advantage that all operations are O(1) and reads do not block writes or each other. ProducerLoadBalancing 生产者将数据直接发送给指定分区Broker集群的Leader Kafka的每一个节点都会维护一份元数据,保存哪些节点是活跃的以及不同topic不同分区的leader Producer Client可以指定数据发送到哪一个分区，机制可以是随机或者用户指定 Asynchronous SendBatching is one of the big drivers of efficiency, and to enable batching the Kafka producer will attempt to accumulate data in memory and to send out larger batches in a single request. The batching can be configured to accumulate no more than a fixed number of messages and to wait no longer than some fixed latency bound (say 64k or 10 ms). This allows the accumulation of more bytes to send, and few larger I/O operations on the servers. This buffering is configurable and gives a mechanism to trade off a small amount of additional latency for better throughput. Details on configuration and the api for the producer can be found elsewhere in the documentation. Consumerpull-based Consumer Position Each Partition is Consumed by one consumer within the consumer group at any given time The Position is just an integer: offset This state can be check-pointed periodicallyOffline DataloadIn the case of Hadoop we parallelize the data load by splitting the load over individual map tasks, one for each node/topic/partition combination, allowing full parallelism in the loading. Hadoop provides the task management, and tasks which fail can restart without danger of duplicate data—they simply restart from their original position. Message Delivery SemanticsProducer Since 0.11.0.0, the Kafka producer also supports an idempotent delivery option which guarantees that resending will not result in duplicate entries in the log. the producer supports the ability to send messages to multiple topic partitions using transaction-like semantics: i.e. either all messages are successfully written or none of them are. If the producer specifies that it wants to wait on the message being committed this can take on the order of 10 ms. However the producer can also specify that it wants to perform the send completely asynchronously it wants to wait only until the leader (but not necessarily the followers) have the message Consumer考虑两种情况： Consumer消费完消息-Crash-&gt;保存offset(重复读取) Consumer保存offset-Crash-&gt;消费完消息(漏读)Kafka的解决方案： write offset &amp; consume消息打包成一个事务，可设置不同的隔离级别 ReplicationThe unit of replication is the topic partitionWe can now more precisely define that a message is considered committed when all in sync replicas for that partition have applied it to their log.Only committed messages are ever given out to the consumer.For Kafka node liveness has two conditionsA node must be able to maintain its session with ZooKeeper (via ZooKeeper’s heartbeat mechanism)If it is a follower it must replicate the writes happening on the leader and not fall “too far” behind Replicated LogsInstead of majority vote, Kafka dynamically maintains a set of in-sync replicas (ISR) that are caught-up to the leader. Only members of this set are eligible for election as leader. A write to a Kafka partition is not considered committed until all in-sync replicas have received the write.Our protocol for allowing a replica to rejoin the ISR ensures that before rejoining, it must fully re-sync again even if it lost unflushed data in its crash.Log Compaction strategy ensures that Kafka will always retain at least the last known value for each message key within the log for a single topic partition.]]></content>
      <categories>
        <category>大数据开发</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java可重入读写锁源码分析]]></title>
    <url>%2F2020%2F01%2F02%2F2020-01-02-Java%E5%8F%AF%E9%87%8D%E5%85%A5%E8%AF%BB%E5%86%99%E9%94%81%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[What is a ReadWriteLockBasically, a ReadWriteLock is designed as a high-level locking mechanism that allows you to add thread-safety feature to a data structure while increasing throughput by allowing multiple threads to read the data concurrently and one thread to update the data exclusively. Performance Goal multiple threads can read the data at the same time as long as there is no thread updating the data Only one thread can update the data at one time, causing other threads(both read and write thread) block until the write lock is released if a thread attempts to update the data while other threads are reading the data, the write lock blocks until the read lock is released. Usage12345ReadWriteLock rwLock = new ReentrantReadWriteLock();//读锁Lock readLock=rwLock.readLock();//写锁Lock writeLock=rwLock.writeLock(); 123try &#123; readLock.lock();&#125;catch... Implementation Detail读写锁共用Sync中的state变量来管理锁状态，高16位为读锁状态，低16位为写锁状态 例子：0000000000000001 | 0000000000000000 0000000000000000 | 0000000000000001 ReadLock（读锁）lock（获取锁的逻辑）读锁获取锁的逻辑:如果写锁没有被其他线程占有，则成功获取锁，否则线程阻塞直到写锁被释放,具体如下 1. lock：调用sync的acquireShared()方法1public void lock() &#123;sync.acquireShared(1);&#125; 2. acquiredShared：调用tryAcquireShared尝试获取共享锁,若获取失败,则调用doAcquireShared阻塞直至被唤醒1234public final void acquireShared(int arg) &#123; if (tryAcquireShared(arg) &lt; 0) doAcquireShared(arg);&#125; 3. tryAcquireShared: 如果当前锁写状态不为0且占锁线程非当前线程，那么返回占锁失败的值-1。 如果公平策略没有要求阻塞且重入数没有到达最大值，则直接尝试cas更新state。 a. 如果cas操作成功，有以下操作逻辑：1）首先，如果当前读锁计数为0那么就设置第一个读线程就是当前线程。 2）其次，当前线程和firstReader同一个线程，记录firstReaderHoldCount也就是第一个读线程读锁定次数。 3）最后，读锁数量不为0并且不为当前线程，获取当前线程ThreadLocal当中的读锁重入计数器。结果返回占锁成功的值1 b. 如果cas操作失败，有以下操作逻辑：通过fullTryAcquireShared尝试获取读锁，内部处理和tryAcquireShared过程相同 4. fullTryAcquireShared逻辑同上，只是加了一个无限循环尝试获取共享锁 5. doAcquireShareddoAcquireShared用来处理读锁获取失败后等待的逻辑,doAcquireShared的内部的自旋保证了线程被唤醒后再次判断是否是第一个节点并尝试获取锁，失败再次进入休眠 1）将当前线程封装成Shared Node类型加入CLH队列 1final Node node = addWaiter(Node.SHARED); 2)如果当前线程的Node节点是CLH队列的第一个节点则当前线程直接获取锁并开启读锁的扩散唤醒所有阻塞读锁的线程 1setHeadAndPropagate(node, r); 3)如果当前线程的Node节点不是CLH队列的第一个节点那么就通过parkAndCheckInterrupt进入休眠 123if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) interrupted = true; unlock（释放锁的逻辑） ReadLock的unlock()方法调用sync.releaseShared(1)方法进行释放。 调用tryReleaseShared()方法尝试释放锁，如果释放成功，调用doReleaseShared尝试唤醒下一个节点 WriteLock（写锁）好累，下次再继续写:&gt;]]></content>
      <categories>
        <category>Java后端开发</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>juc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Summary Of Data Structure]]></title>
    <url>%2F2019%2F10%2F14%2F2019-10-14-Summary%20of%20Data%20Structure%2F</url>
    <content type="text"><![CDATA[PrefaceSummarize what I learned from the data structure class and some of its implementation in different language. Category Array LinkedList Stack Queue Heap Map HashMap/Dictionary LinkedHashMap/OrderedDict Tree Binary Search Tree B tree B-tree B+tree B* tree Red-Black Tree Graph Quick Find Quick Union Weighted Quick Union Path Compression Weighted + Path ArrayStructure Member Function Time Complexity Action Worst Time Complexity Add O(1) Access By Index O(1) Search O(N) Insert O(N) Resize O(N) Delete O(N) Implementation in different languageC++ Array—Fixed Size In StackExample: 1234567891011121314#include &lt;iostream&gt;using namespace std;int main()&#123; int arr[]=&#123;10,20,40,60&#125;; int *p=arr; cout&lt;&lt;arr&lt;&lt;endl; cout&lt;&lt;arr+1&lt;&lt;endl; cout&lt;&lt;*p&lt;&lt;endl; cout&lt;&lt;*(p+2)&lt;&lt;endl; cout&lt;&lt;p[3]&lt;&lt;endl; cout&lt;&lt;&amp;p&lt;&lt;endl; return 0;&#125; Dynamic Array–Can Change Size During Runtime 12345678910int* a = NULL; // Pointer to int, initialize to nothing.int n; // Size needed for arraycin &gt;&gt; n; // Read in the sizea = new int[n]; // Allocate n ints and save ptr in a.for (int i=0; i&lt;n; i++) &#123; a[i] = 0; // Initialize all elements to zero.&#125;delete [] a; // When done, free memory pointed to by a.a = NULL; // Clear a to prevent using invalid memory reference. Java Plain ArrayMuch Similar As what is done in C++ ArrayList(Defined in JDK library) Def: An Object Class that implements List InterfaceNote that ArrayList used static function of Arrays to do the resize 12345678910public static &lt;T,U&gt; T[] copyOf(U[] original, int newLength, Class&lt;? extends T[]&gt; newType)&#123; @SuppressWarnings("unchecked") T[] copy = ((Object)newType == (Object)Object[].class) ? (T[]) new Object[newLength] : (T[]) Array.newInstance(newType.getComponentType(), newLength);System.arraycopy(original, 0, copy, 0, Math.min(original.length, newLength));return copy;&#125; The Java Virtual Machine used reflection mechanism to initialize a new Array object with the new size during runtime Copy the old element to the new Array PythonIn python, one can use list to serve as an array Linked ListStructure Member Function Time Complexity Action Worst Time Complexity Add O(1) Access By Index O(N) Search O(N) Insert O(N) Resize O(1) Delete O(N) Implementation in different languageC++Example: 123456789101112131415161718192021222324252627282930313233343536373839#include &lt;iostream&gt;using namespace std;struct node&#123; int data; node *next;&#125;;class linked_list&#123;private: node *head,*tail;public: linked_list() &#123; head = NULL; tail = NULL; &#125; void add_node(int n) &#123; node *tmp = new node; tmp-&gt;data = n; tmp-&gt;next = NULL; if(head == NULL) &#123; head = tmp; tail = tmp; &#125; else &#123; tail-&gt;next = tmp; tail = tail-&gt;next; &#125; &#125;&#125;; JavaPredefined LinkedList that implements List interface PythonDef Node 1234567891011121314class Node(object): def __init__(self, data=None, next_node=None): self.data = data self.next_node = next_node def get_data(self): return self.data def get_next(self): return self.next_node def set_next(self, new_next): self.next_node = new_next Def linkedList 12345class LinkedList(object): def __init__(self, head=None): self.head = head #...omitted# StackFILO: First In Last Out Structure Member Function Time Complexity Action Worst Time Complexity Push O(1) Pop O(1) Implementation in Different LanguageC++12345678910111213141516171819202122232425#ifndef STACK_H#define STACK_H#include &lt;cstdlib&gt;#include "node.h"using namespace std;template &lt;class T&gt;class stack&#123; public: // constructor stack(); // modifiers void push(T entry); void pop(); // accessors bool empty(); T top(); private: node&lt;T&gt;* head;&#125;;#include "stack.cpp"#endif JavaCan use ArrayList to serve as Stack PythonCan use list QueueFIFO:First In Last Out Structure Heap MinHeap MaxHeapCan be implemented using Array Time Complexity Action Worst Time Complexity Step insert O(logN) NA delete min O(logN) Switch min with the last entry and heapify Sort O(NlogN) build a min heap, delete min iteratively MapStructure Hashing Probing Linear Probing: When Collision happens, always find the nex spot Quadratic Probing: When Collision happens, always find the next i^2 spots Chained Hashing When collision happens, Link the same hash element Implementation Python Dict OrderedDict Java HashMap LinkedHashMap Tree Binary Tree Full Binary Tree Complete Binary Tree Consider the Skewed Case Action Worst Time Complexity insert O(N) delete O(N) search O(N) B tree（Self Balanced） Action Worst Time Complexity insert O(logN) delete O(logN) search O(logN) Difference Between different B tree B-tree: Self Balanced Search Tree B+tree: Only the leaf Node store the data, Non-leaf node only contains the key as road map B* tree: Contains the pointer to brother node Red-Black Tree Root of the tree is always black No adjacent red node Every path from a node to any of its descendent null node has the same number of black node GraphWith M union and find Operation on N graph Node objects Time Complexity type of union-find Worst Time Complexity Quick-find MN Quick-union MN weighted Union N+MlogN Path Compression N+MlogN weighted+Path (M+N)log* N]]></content>
      <categories>
        <category>计算机基础</category>
      </categories>
      <tags>
        <tag>data structure</tag>
        <tag>linked list</tag>
        <tag>array</tag>
        <tag>hash</tag>
        <tag>graph</tag>
        <tag>stack</tag>
        <tag>queue</tag>
        <tag>heap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Cache]]></title>
    <url>%2F2019%2F09%2F10%2F2019-09-10-Spring%20Cache%2F</url>
    <content type="text"><![CDATA[前言：这几天做公司项目的时候第一次接触CaffineCache,一开始写的是手动操作的版本，近几日做优化利用了Spring集成的接口来进行管理 Spring Cache是什么官方文档 the Spring Framework provides support for transparently adding caching to an existing Spring application. Similar to the transaction support, the caching abstraction allows consistent use of various caching solutions with minimal impact on the code. 简单来说,Spring定义了一组规范的CRUD API规范,使我们可以方便的操作缓存实体(Redis,Caffine等) 如何配置application.properties1234# 定义缓存名称spring.cache.cache-names=agencyDataIndexOption,realTimeIndex# 设定延时，最大数据量spring.cache.caffeine.spec=maximumSize=500,expireAfterAccess=6s 配置类配置多个CacheManager: 可以为每个Cache定义不同的CacheManager 可以设定不同的时长 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748@Configuration@EnableCaching@EnableConfigurationProperties(CacheProperties.class)public class CacheManagerConfiguration &#123; private final CacheProperties cacheProperties; public CacheManagerConfiguration(CacheProperties cacheProperties) &#123; this.cacheProperties = cacheProperties; &#125; public interface CacheManagerNames &#123; String REDIS_CACHE_MANAGER = "redisCacheManager"; String EHCACHE_CACHE_MANAGER = "ehCacheManager"; &#125; @Bean(name = CacheManagerNames.REDIS_CACHE_MANAGER) public RedisCacheManager redisCacheManager(RedisConnectionFactory factory) &#123; Map&lt;String, RedisCacheConfiguration&gt; expires = ImmutableMap.&lt;String, RedisCacheConfiguration&gt;builder() .put("15", RedisCacheConfiguration.defaultCacheConfig().entryTtl( Duration.ofMillis(15) )) .put("30", RedisCacheConfiguration.defaultCacheConfig().entryTtl( Duration.ofMillis(30) )) .put("60", RedisCacheConfiguration.defaultCacheConfig().entryTtl( Duration.ofMillis(60) )) .put("120", RedisCacheConfiguration.defaultCacheConfig().entryTtl( Duration.ofMillis(120) )) .build(); RedisCacheManager redisCacheManager = RedisCacheManager.RedisCacheManagerBuilder.fromConnectionFactory(factory) .withInitialCacheConfigurations(expires) .build(); return redisCacheManager; &#125; @Bean(name = CacheManagerNames.EHCACHE_CACHE_MANAGER) @Primary public EhCacheCacheManager ehCacheManager() &#123; Resource resource = this.cacheProperties.getEhcache().getConfig(); resource = this.cacheProperties.resolveConfigLocation(resource); EhCacheCacheManager ehCacheManager = new EhCacheCacheManager( EhCacheManagerUtils.buildCacheManager(resource) ); ehCacheManager.afterPropertiesSet(); return ehCacheManager; &#125;&#125; 使用(CaffineCache为例)三个注解 @Cacheable: Triggers cache population.(当重复使用相同参数调用方法的时候，方法本身不会被调用执行，即方法本身被略过了，取而代之的是方法的结果直接从缓存中找到并返回了) @CacheEvict: Triggers cache eviction. @CachePut: Updates the cache without interfering with the method execution.(这个注释可以确保方法被执行，同时方法的返回值也被记录到缓存中。) 代码实例在Service中使用1234567891011@Cacheable(cacheNames = "businessIndex", sync = true) @Override public BseBusinessIndexPO getCachedIndex(Integer id) &#123; return getIndexById(id); &#125; @Cacheable(cacheNames = "businessIndex", sync = true) @Override public BseBusinessIndexPO getCachedIndex(String tableName, String fieldName) &#123; return businessIndexMapper.selectByTableAndField(tableName, fieldName); &#125; 手动管理缓存,利用Spring默认的CacheManager12345678910111213@GetMapping("/&#123;cacheName&#125;/&#123;cacheKey&#125;/evict") public WebApiResponse&lt;String&gt; evict(@PathVariable("cacheName") String cacheName, @PathVariable("cacheKey") String cacheKey) &#123; try &#123; Object key=generateKey(cacheKey); Optional.ofNullable(cacheManager.getCache(cacheName)) .ifPresent(k -&gt; k.evict(key)); return WebApiResponse.success( String.format("缓存cacheName: %s at %s清除成功", cacheName, (String) cacheKey)); &#125; catch (Exception e) &#123; return WebApiResponse.error(e.getMessage()); &#125; &#125; reference:https://docs.spring.io/spring/docs/5.1.9.RELEASE/spring-framework-reference/integration.html#cachehttps://docs.spring.io/spring-boot/docs/current/reference/html/boot-features-caching.html]]></content>
      <categories>
        <category>Java后端开发</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>spring</tag>
        <tag>cache</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Cache]]></title>
    <url>%2F2019%2F09%2F10%2F2019-09-10-SpringCache%2F</url>
    <content type="text"><![CDATA[前言：这几天做公司项目的时候第一次接触CaffineCache,一开始写的是手动操作的版本，近几日做优化利用了Spring集成的接口来进行管理Spring Cache是什么官方文档 the Spring Framework provides support for transparently adding caching to an existing Spring application. Similar to the transaction support, the caching abstraction allows consistent use of various caching solutions with minimal impact on the code. 简单来说,Spring定义了一组规范的CRUD API规范,使我们可以方便的操作缓存实体(Redis,Caffine等) 如何配置application.properties1234# 定义缓存名称spring.cache.cache-names=agencyDataIndexOption,realTimeIndex# 设定延时，最大数据量spring.cache.caffeine.spec=maximumSize=500,expireAfterAccess=6s 配置类配置多个CacheManager: 可以为每个Cache定义不同的CacheManager 可以设定不同的时长 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748@Configuration@EnableCaching@EnableConfigurationProperties(CacheProperties.class)public class CacheManagerConfiguration &#123; private final CacheProperties cacheProperties; public CacheManagerConfiguration(CacheProperties cacheProperties) &#123; this.cacheProperties = cacheProperties; &#125; public interface CacheManagerNames &#123; String REDIS_CACHE_MANAGER = "redisCacheManager"; String EHCACHE_CACHE_MANAGER = "ehCacheManager"; &#125; @Bean(name = CacheManagerNames.REDIS_CACHE_MANAGER) public RedisCacheManager redisCacheManager(RedisConnectionFactory factory) &#123; Map&lt;String, RedisCacheConfiguration&gt; expires = ImmutableMap.&lt;String, RedisCacheConfiguration&gt;builder() .put("15", RedisCacheConfiguration.defaultCacheConfig().entryTtl( Duration.ofMillis(15) )) .put("30", RedisCacheConfiguration.defaultCacheConfig().entryTtl( Duration.ofMillis(30) )) .put("60", RedisCacheConfiguration.defaultCacheConfig().entryTtl( Duration.ofMillis(60) )) .put("120", RedisCacheConfiguration.defaultCacheConfig().entryTtl( Duration.ofMillis(120) )) .build(); RedisCacheManager redisCacheManager = RedisCacheManager.RedisCacheManagerBuilder.fromConnectionFactory(factory) .withInitialCacheConfigurations(expires) .build(); return redisCacheManager; &#125; @Bean(name = CacheManagerNames.EHCACHE_CACHE_MANAGER) @Primary public EhCacheCacheManager ehCacheManager() &#123; Resource resource = this.cacheProperties.getEhcache().getConfig(); resource = this.cacheProperties.resolveConfigLocation(resource); EhCacheCacheManager ehCacheManager = new EhCacheCacheManager( EhCacheManagerUtils.buildCacheManager(resource) ); ehCacheManager.afterPropertiesSet(); return ehCacheManager; &#125;&#125; 使用(CaffineCache为例)三个注解 @Cacheable: Triggers cache population.(当重复使用相同参数调用方法的时候，方法本身不会被调用执行，即方法本身被略过了，取而代之的是方法的结果直接从缓存中找到并返回了) @CacheEvict: Triggers cache eviction. @CachePut: Updates the cache without interfering with the method execution.(这个注释可以确保方法被执行，同时方法的返回值也被记录到缓存中。)代码实例在Service中使用1234567891011@Cacheable(cacheNames = "businessIndex", sync = true) @Override public BseBusinessIndexPO getCachedIndex(Integer id) &#123; return getIndexById(id); &#125; @Cacheable(cacheNames = "businessIndex", sync = true) @Override public BseBusinessIndexPO getCachedIndex(String tableName, String fieldName) &#123; return businessIndexMapper.selectByTableAndField(tableName, fieldName); &#125; 手动管理缓存,利用Spring默认的CacheManager 12345678910111213@GetMapping("/&#123;cacheName&#125;/&#123;cacheKey&#125;/evict") public WebApiResponse&lt;String&gt; evict(@PathVariable("cacheName") String cacheName, @PathVariable("cacheKey") String cacheKey) &#123; try &#123; Object key=generateKey(cacheKey); Optional.ofNullable(cacheManager.getCache(cacheName)) .ifPresent(k -&gt; k.evict(key)); return WebApiResponse.success( String.format("缓存cacheName: %s at %s清除成功", cacheName, (String) cacheKey)); &#125; catch (Exception e) &#123; return WebApiResponse.error(e.getMessage()); &#125; &#125; reference:https://docs.spring.io/spring/docs/5.1.9.RELEASE/spring-framework-reference/integration.html#cachehttps://docs.spring.io/spring-boot/docs/current/reference/html/boot-features-caching.html]]></content>
      <categories>
        <category>Java后端开发</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>spring</tag>
        <tag>cache</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringIOC容器的启动流程(一)]]></title>
    <url>%2F2019%2F09%2F07%2F2019-09-06-SpringIOC%E5%AE%B9%E5%99%A8%E7%9A%84%E5%90%AF%E5%8A%A8%E6%B5%81%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[前言:Spring做为JAVA企业级应用中最为热门的框架,为开发人员提供了一系列方便易用的组件,使开发者可以集中于业务逻辑的开发，而不必关心太多底层环境的部署。我第一次接触这个框架是做校内的一个web项目，当时只是简单机械的运用了spring-boot 中的一些功能，正式开始了解框架背后的原理是今年暑假实习。一开始，我被Spring官方晦涩的文档所困住，理不清思绪，跨不出学习的第一步。 稻圣和夫曾经说过,真理之布由一根纱线织成,把事情看得越单纯，就越接近真相，也就越接近真理。一句话概括，Spring是一个轻量级控制反转(IoC)和面向切面(AOP)的容器框架。因此,我们不妨从Spring框架最核心的两个思想IOC,AOP开始，慢慢探寻框架背后的秘密。 本篇博客将以一个简单的Spring应用程序为例，介绍Spring的IOC容器启动流程 IOC容器启动步骤概括 定位配置文件(xml或configuration Class) 加载配置文件获取BeanDefinition 注册BeanDefinition 实例化Bean 简单的Spring程序我们首先来看一个简单的HelloWorld程序,调用的我们的Renderer并渲染相应的消息 主程序123456789public class HelloWordApplication &#123; public static void main(String args[])&#123; ApplicationContext ctx= new ClassPathXmlApplicationContext("/app-context.xml"); MessageRenderer renderer=ctx.getBean("renderer",MessageRenderer.class); renderer.renderer(); &#125;&#125; 相关接口和Bean 消息提供方 1234567public class HelloWordMessageProvider implements MessageProvider &#123; @Override public String getMessage() &#123; return "HelloWord!"; &#125;&#125; 1234public interface MessageProvider &#123; String getMessage();&#125; 渲染消息方 123456public interface MessageRenderer &#123; void renderer(); void setMessageProvider(MessageProvider messageProvider); MessageProvider getMessageProvider();&#125; 12345678910111213141516171819202122public class StandardMessageRenderer implements MessageRenderer &#123; private MessageProvider messageProvider; @Override public void renderer() &#123; if (messageProvider==null)&#123; throw new RuntimeException("error"); &#125; System.out.println(messageProvider.getMessage()); &#125; @Override public void setMessageProvider(MessageProvider messageProvider) &#123; this.messageProvider = messageProvider; &#125; @Override public MessageProvider getMessageProvider() &#123; return this.messageProvider; &#125;&#125; 从源码分析IOC容器启动首先我们来看一看主程序里的这一行 1ApplicationContext ctx= new ClassPathXmlApplicationContext("/app-context.xml"); 短短的一行,确已经完成了IOC容器的启动,我们一会再去探寻这个构造函数到底做了什么,先来看看ApplicationContext究竟是什么 BeanFactory vs ApplicationContextBeanFactoryBeanFactory是我们的根容器,Bean的中央注册中心,提供了依赖注入,ApplicationContext继承了BeanFactory,一般我们不会直接操作BeanFactory,而都是直接和ApplicationContext打交道 java doc上给出的BeanFactory的定义 The root interface for accessing a Spring bean container. This is the basic client view of a bean container; further interfaces such as ListableBeanFactory and ConfigurableBeanFactory are available for specific purposes. 这边给出一个例子演示一下如何直接通过BeanFactory操纵我们的Bean MessageRenderer 12345678910public class XmlConfigWithBeanFactory &#123; public static void main(String[] args) &#123; // DefaultListableBeanFactory factory=new DefaultListableBeanFactory(); XmlBeanDefinitionReader rdr=new XmlBeanDefinitionReader(factory); rdr.loadBeanDefinitions(new ClassPathResource("/app-context.xml")); MessageRenderer renderer=factory.getBean("renderer",MessageRenderer.class); renderer.renderer(); &#125;&#125; DefaultListableBeanFactory是实现了ListableBeanFactory接口的一个类1.初始化factory2.使用XmlBeanDefinitionReader从Xml文件读取BeanDefinition信息3.利用我们的factory获取我们定义好的Bean ApplicationContextApplicationContext是BeanFactory的扩展,除了依赖注入外,其还提供了 Easier integration with Spring’s AOP features Message resource handling (for use in internationalization) Event publication Application-layer specific contexts such as the WebApplicationContext for use in web applications. 源码解析1ApplicationContext ctx= new ClassPathXmlApplicationContext("/app-context.xml"); ClassPathXmlApplicationContext简单来说就是根据xml文件来获取我们的Bean的定义,并将Bean注册到我们的容器中 根据源码追踪我们会看到 123public ClassPathXmlApplicationContext(String... configLocations) throws BeansException &#123; this(configLocations, true, (ApplicationContext)null);&#125; 这里就是我们真正的构造函数 123456789101112public ClassPathXmlApplicationContext(String[] paths, Class&lt;?&gt; clazz, @Nullable ApplicationContext parent) throws BeansException &#123; super(parent); Assert.notNull(paths, "Path array must not be null"); Assert.notNull(clazz, "Class argument must not be null"); this.configResources = new Resource[paths.length]; for(int i = 0; i &lt; paths.length; ++i) &#123; this.configResources[i] = new ClassPathResource(paths[i], clazz); &#125; this.refresh();&#125; 首先我们将父容器设为null(关于父子容器这块到时候单独研究下) set我们的BeanDefinition 调用refresh(),容器启动的真正入口 容器启动的入口: refresh源码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263public void refresh() throws BeansException, IllegalStateException &#123; synchronized (this.startupShutdownMonitor) &#123; // Prepare this context for refreshing. prepareRefresh(); // Tell the subclass to refresh the internal bean factory. ConfigurableListableBeanFactory beanFactory = obtainFreshBeanFactory(); // Prepare the bean factory for use in this context. prepareBeanFactory(beanFactory); try &#123; // Allows post-processing of the bean factory in context subclasses. postProcessBeanFactory(beanFactory); // Invoke factory processors registered as beans in the context. invokeBeanFactoryPostProcessors(beanFactory); // Register bean processors that intercept bean creation. registerBeanPostProcessors(beanFactory); // Initialize message source for this context. initMessageSource(); // Initialize event multicaster for this context. initApplicationEventMulticaster(); // Initialize other special beans in specific context subclasses. onRefresh(); // Check for listener beans and register them. registerListeners(); // Instantiate all remaining (non-lazy-init) singletons. finishBeanFactoryInitialization(beanFactory); // Last step: publish corresponding event. finishRefresh(); &#125; catch (BeansException ex) &#123; if (logger.isWarnEnabled()) &#123; logger.warn("Exception encountered during context initialization - " + "cancelling refresh attempt: " + ex); &#125; // Destroy already created singletons to avoid dangling resources. destroyBeans(); // Reset 'active' flag. cancelRefresh(ex); // Propagate exception to caller. throw ex; &#125; finally &#123; // Reset common introspection caches in Spring's core, since we // might not ever need metadata for singleton beans anymore... resetCommonCaches(); &#125; &#125; &#125; 拆解一下，概括一下这个函数做的事情 graph TD A[prepareRefresh:容器启动前的准备工作]-->B[prepareBeanFactory:容器核心初始化] B-->C[invokeBeanFactoryPostProcessor:初始化BeanPostProcesor Bean] C-->D[finishBeanFactoryInitializatio:创建Singleton的Bean] D-->E[finishRefresh] 后记就先写到这里吧:&gt;,下一篇写一下SpringBoot启动MVC的流程吧,嗷呜,感觉给自己挖了一个大坑]]></content>
      <categories>
        <category>Java后端开发</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>spring</tag>
        <tag>ioc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java动态代理源码解析]]></title>
    <url>%2F2019%2F09%2F02%2F2019-09-02-Java%E5%8A%A8%E6%80%81%E4%BB%A3%E7%90%86%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%2F</url>
    <content type="text"><![CDATA[前言: 动态代理是Java高级编程思想中比较重要的一块，《Java核心技术：卷一》中称其为系统设计开发者必须掌握的一门技术。JAVA RMI API（远程方法调用）中就利用了动态代理的思想实现了客户端代理类与服务端代理类之间的直接交互，而开发者并不需要了解底层的连接细节，实现了代码的解耦，让开发者可以专注业务逻辑的实现而无需关心底层细节。本文将首先给出动态代理的机制，接着结合一个动态代理的实现例子来追踪分析源码。 一. 动态代理机制 代理是一种常用的设计模式，其目的就是为其他对象提供一个代理以控制对某个对象的访问。代理类负责为委托类预处理消息，过滤消息并转发消息，以及进行消息被委托类执行后的后续处理[^1] [^1]:Java 动态代理机制分析及扩展，第 1 部分 预定义一个实际类RealSubject,并让其实现某个接口Subject中的某个方法x，该方法即我们的代理类需要调用的 定义一个ProxyHandler implements InvocationHandler 利用Proxy的静态方法newProxyInstance生成代理对象 通过代理对象调用方法x 二. 例子Dynamic Proxy Test 12345678910111213141516171819package dynamic;import java.lang.reflect.Proxy;/** * 动态代理 * * @author xcw */public class DynamicProxy &#123; public static void main(String[] args) &#123; RealSubject realSubject = new RealSubject(); //1.创建委托对象 ProxyHandler handler = new ProxyHandler(realSubject); //2.创建调用处理器对象 Subject proxySubject = (Subject) Proxy.newProxyInstance(RealSubject.class.getClassLoader(), RealSubject.class.getInterfaces(), handler); //3.动态生成代理对象 proxySubject.request(); //4.通过代理对象调用方法 &#125;&#125; RealSubject 12345678910111213package dynamic;/** * 实际类 * * @author xcw */public class RealSubject implements Subject &#123; public void request() &#123; System.out.println("====RealSubject Request===="); &#125;&#125; ProxyHandler 12345678910111213141516171819202122232425package dynamic;import java.lang.reflect.InvocationHandler;import java.lang.reflect.Method;/** * TODO 类描述 * * @author xcw */public class ProxyHandler implements InvocationHandler &#123; private Subject subject; public ProxyHandler(Subject subject)&#123; this.subject = subject; &#125; @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; System.out.println("====before====");//定义预处理的工作，当然你也可以根据 method 的不同进行不同的预处理工作 Object result = method.invoke(subject, args); System.out.println("====after===="); return result; &#125;&#125; Subject接口 1234567891011package dynamic;/** * Subject接口 * * @author xcw */public interface Subject &#123; void request();&#125; 三. 源码解析创建调用处理器对象1.定义ProxyHandler1ProxyHandler handler = new ProxyHandler(realSubject); //2.创建调用处理器对象 2.ProxyHandler implements 了InvocationHanler这个接口,我们先来了解一下InvocationHandler是什么官方注释 InvocationHandler is the interface implemented by the invocation handler of a proxy instance.Each proxy instance has an associated invocation handler. When a method is invoked on a proxy instance, the method invocation is encoded and dispatched to the invoke method of its invocation handler. 这段话翻译一下就是每一个代理对象在调用某个方法时，它的本质都是调用了InvocationHandler其中的invoke方法 3.接下来我们着重看一下InvocationHanler接口下的invoke这个方法首先是官方文档对其的描述： Processes a method invocation on a proxy instance and returns the result. This method will be invoked on an invocation handler when a method is invoked on a proxy instance that it is associated with. 解释一下传入的三个参数: proxy: the proxy instance that the method was invoked on (代理) method: the Method instance corresponding to the interface method invoked on the proxy instance(java.lang.Method提供了关于某个方法的一系列信息) args: an array of objects containing the values of the arguments passed in the method invocation on the proxy instance(某个参数需要传入的argument的数组 type 为 Object[]) 4. 我们看一下我们对invoke这个函数究竟做了什么首先我们为它自定义了一系列预处理操作,然后我们调用了method.invoke方法执行了ReaSubject中我们想要访问的方法x,最后我们又做了一系列收尾工作,愉快的退出了我们的invoke方法 12345678@Overridepublic Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; System.out.println("====before====");//定义预处理的工作，当然你也可以根据 method 的不同进行不同的预处理工作 Object result = method.invoke(subject, args); System.out.println("====after===="); return result; &#125; 接下来是重头戏, method.invoke(subject,args),注意,这里的invoke方法是定义在Method Class 下的一个instance function 1Object result = method.invoke(subject, args); 1234567891011121314151617@CallerSensitive public Object invoke(Object obj, Object... args) throws IllegalAccessException, IllegalArgumentException, InvocationTargetException &#123; if (!override) &#123; if (!Reflection.quickCheckMemberAccess(clazz, modifiers)) &#123; Class&lt;?&gt; caller = Reflection.getCallerClass(); checkAccess(caller, clazz, obj, modifiers); &#125; &#125; MethodAccessor ma = methodAccessor; // read volatile if (ma == null) &#123; ma = acquireMethodAccessor(); &#125; return ma.invoke(obj, args); &#125; 这段代码干的事情简单来说就是通过反射的思想拿到了此method的caller class,并执行方法.当然如果深究下去,就涉及到许多反射的知识了,所以在这里掠过,之后单独写一篇关于反射原理的吧:&gt; 动态生成代理对象12Subject proxySubject = (Subject) Proxy.newProxyInstance(RealSubject.class.getClassLoader(), RealSubject.class.getInterfaces(), handler); //3.动态生成代理对象 1.Proxy中的静态方法newProxyInstance为我们返回了一个代理对象2.我们看一下传入的三个参数 loader: the class loader to define the proxy class interfaces: the list of interfaces for the proxy class to implement h: the invocation handler to dispatch method invocations to 3.我们把源码拆解一下,这个函数主要做的操作123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051@CallerSensitive public static Object newProxyInstance(ClassLoader loader, Class&lt;?&gt;[] interfaces, InvocationHandler h) throws IllegalArgumentException &#123; Objects.requireNonNull(h); final Class&lt;?&gt;[] intfs = interfaces.clone(); final SecurityManager sm = System.getSecurityManager(); if (sm != null) &#123; checkProxyAccess(Reflection.getCallerClass(), loader, intfs); &#125; /* * Look up or generate the designated proxy class. */ Class&lt;?&gt; cl = getProxyClass0(loader, intfs); /* * Invoke its constructor with the designated invocation handler. */ try &#123; if (sm != null) &#123; checkNewProxyPermission(Reflection.getCallerClass(), cl); &#125; final Constructor&lt;?&gt; cons = cl.getConstructor(constructorParams); final InvocationHandler ih = h; if (!Modifier.isPublic(cl.getModifiers())) &#123; AccessController.doPrivileged(new PrivilegedAction&lt;Void&gt;() &#123; public Void run() &#123; cons.setAccessible(true); return null; &#125; &#125;); &#125; return cons.newInstance(new Object[]&#123;h&#125;); &#125; catch (IllegalAccessException|InstantiationException e) &#123; throw new InternalError(e.toString(), e); &#125; catch (InvocationTargetException e) &#123; Throwable t = e.getCause(); if (t instanceof RuntimeException) &#123; throw (RuntimeException) t; &#125; else &#123; throw new InternalError(t.toString(), t); &#125; &#125; catch (NoSuchMethodException e) &#123; throw new InternalError(e.toString(), e); &#125; &#125; 1.我们首先利用getProxyClass0拿到了我们需要的代理class,type Class 123//Look up or generate the designated proxy class.Class&lt;?&gt; cl = getProxyClass0(loader, intfs); 2.接着我们通过代理class拿到了我们代理类的构造函数 1final Constructor&lt;?&gt; cons = cl.getConstructor(constructorParams); 3.最后我们愉快的通过反射构造了一个代理类 1return cons.newInstance(new Object[]&#123;h&#125;); 注意这里我们传入了h,也就是我们之前自己定义的实现InvocationHanler的ProxyHandler,看到这一步的时候顿时有一种茅塞顿开的感觉,为什么这么说呢,因为其实我们知道,所有生成的代理类,无论是静态还是动态,它都必须继承我们的Proxy类,然后我们来看一下Proxy类的源码 1protected InvocationHandler h; 这里定义了我们的小h,然后我们再来看一看Proxy类的构造函数 1234567891011121314/** * Constructs a new &#123;@code Proxy&#125; instance from a subclass * (typically, a dynamic proxy class) with the specified value * for its invocation handler. * * @param h the invocation handler for this proxy instance * * @throws NullPointerException if the given invocation handler, &#123;@code h&#125;, * is &#123;@code null&#125;. */ protected Proxy(InvocationHandler h) &#123; Objects.requireNonNull(h); this.h = h; &#125; 没错,到这一步,我们终于走完了我们所有的流程,也就是我们终于构造了我们的动态代理对象,并且我们也知道了我们在调用x方法时jvm所经历的所有路程,其实这里还遗留了一点magic,就是我们的动态代理类它到底长啥样呢,这里由于是jvm底层做的操作,源码并没有能追踪到,不过我们可以大致想象出来,这边我直接粘贴一下某位大佬的代码了^2 1234567891011public final class $Proxy1 extends Proxy implements Subject&#123; private InvocationHandler h; private $Proxy1()&#123;&#125; public $Proxy1(InvocationHandler h)&#123; this.h = h; &#125; public int request(int i)&#123; Method method = Subject.class.getMethod("request", new Class[]&#123;int.class&#125;); //创建method对象 return (Integer)h.invoke(this, method, new Object[]&#123;new Integer(i)&#125;); //调用了invoke方法 &#125;&#125; 四. 总结其实追踪源码的过程还是挺枯燥又费时的,并且涉及到的java反射知识繁多复杂,但一旦理清整体的脉络就会有一种茅塞顿开的感觉,它能帮助你更好的理解一些框架的底层原理,同时也深刻意识到Java反射大法好,还要继续学习:&gt;]]></content>
      <categories>
        <category>Java后端开发</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>proxy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JAVA rmi的实现及源码解析]]></title>
    <url>%2F2019%2F09%2F01%2F2019-09-01-JAVA%20rmi%E7%9A%84%E5%AE%9E%E7%8E%B0%E5%8F%8A%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%2F</url>
    <content type="text"><![CDATA[前言:rmi(remote method invocation)是java官方的远程调用的是一种实现方式，它使得我们能像调用本地服务一般调用远程服务 源码分析1. 客户端调用代码123456789101112131415161718192021222324252627282930import java.rmi.Naming;/** * TODO 类描述 * * @author xcw */public class ClientApplication &#123; public static void main(String args[]) &#123; String url = "rmi://localhost:8888/"; try &#123; // 在RMI服务注册表中查找名称为server-service的对象，并调用其上的方法 //Naming.lookup(registry url+service name)将会返回Remote接口 ServerService service = (ServerService) Naming.lookup(url + "server-service"); Class stubClass = service.getClass(); System.out.println(service + " 是 " + stubClass.getName() + " 的实例！"); // 获得本底存根已实现的接口类型 Class[] interfaces = stubClass.getInterfaces(); for (Class c : interfaces) &#123; System.out.println("存根类实现了 " + c.getName() + " 接口！"); &#125; System.out.println(service.service()); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125; 分析1. 在RMI服务注册表中查找服务接口, Naming.lookup(registry url+service name)将会返回Proxy[ServerService,RemoteObjectInvocationHandler[UnicastRef [liveRef: [endpoint:192.168.1.103:49471,objID:[-79051ded:16cec85dc68:-7fff, -7771089289704678398]]]]] 即ServerService的动态代理对象 1ServerService service = (ServerService) Naming.lookup(url + "server-service"); 2. 进入Naming.lookup方程12345678910111213141516171819202122232425/** * Returns a reference, a stub, for the remote object associated * with the specified &lt;code&gt;name&lt;/code&gt;. * * @param name a name in URL format (without the scheme component) * @return a reference for a remote object * @exception NotBoundException if name is not currently bound * @exception RemoteException if registry could not be contacted * @exception AccessException if this operation is not permitted * @exception MalformedURLException if the name is not an appropriately * formatted URL * @since JDK1.1 */ public static Remote lookup(String name) throws NotBoundException, java.net.MalformedURLException, RemoteException &#123; ParsedNamingURL parsed = parseURL(name); Registry registry = getRegistry(parsed); if (parsed.name == null) return registry; return registry.lookup(parsed.name); &#125; 1Registry registry = getRegistry(parsed); Registry is a remote interface to a simple remote object registry that provides methods for storing and retrieving remote object references bound with arbitrary string names. 3. Registry接口帮我们拿到我们想要的指定名称的远程服务的reference 即RegistryImpl_Stub 进入registry.lookup(parsed.name) 调用 RegistryImpl_Stub的ref（RemoteRef）对象的newCall()方法，将RegistryImpl_Stub对象传了进去，不要忘了构造它的时候我们将服务器的主机端口等信息传了进去，也就是我们把服务器相关的信息也传进了newCall()方法。newCall()方法做的事情简单来看就是建立了跟远程RegistryImpl的Skeleton对象的连接 123456789101112131415161718192021222324252627282930313233343536public Remote lookup(String var1) throws AccessException, NotBoundException, RemoteException &#123; try &#123; RemoteCall var2 = this.ref.newCall(this, operations, 2, 4905912898345647071L); try &#123; ObjectOutput var3 = var2.getOutputStream(); var3.writeObject(var1); &#125; catch (IOException var17) &#123; throw new MarshalException("error marshalling arguments", var17); &#125; this.ref.invoke(var2); Remote var22; try &#123; ObjectInput var4 = var2.getInputStream(); var22 = (Remote)var4.readObject(); &#125; catch (IOException var14) &#123; throw new UnmarshalException("error unmarshalling return", var14); &#125; catch (ClassNotFoundException var15) &#123; throw new UnmarshalException("error unmarshalling return", var15); &#125; finally &#123; this.ref.done(var2); &#125; return var22; &#125; catch (RuntimeException var18) &#123; throw var18; &#125; catch (RemoteException var19) &#123; throw var19; &#125; catch (NotBoundException var20) &#123; throw var20; &#125; catch (Exception var21) &#123; throw new UnexpectedException("undeclared checked exception", var21); &#125; &#125; 4. 客户端获取服务端返回的服务Stub对象，接下来可以利用Stub对象进行远程调用2. 服务端123456789101112131415161718192021222324252627282930import java.rmi.Naming;import java.rmi.registry.LocateRegistry;/** * TODO 类描述 * * @author xcw */public class ServerApplication &#123; public static void main(String args[]) &#123; try &#123; //实例化ServerServiceImpl ServerService service = new ServerServiceImpl(); // 本地主机上的远程对象注册表Registry的实例，并指定端口为8888，这一步必不可少（Java默认端口是1099 // ），必不可缺的一步，缺少注册表创建，则无法绑定对象到远程注册表上 LocateRegistry.createRegistry(8888); //绑定的URL标准格式为：rmi://host:port/name(其中协议名可以省略，下面两种写法都是正确的） Naming.bind("rmi://localhost:8888/server-service", service); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; System.out.println("服务器向命名表注册了1个远程服务对象！"); &#125;&#125; 1. LocateRegistry.createRegistry(8888)创建RegistryImpl对象源码 12345678910111213141516171819202122public RegistryImpl(final int var1) throws RemoteException &#123; this.bindings = new Hashtable(101); if (var1 == 1099 &amp;&amp; System.getSecurityManager() != null) &#123; try &#123; AccessController.doPrivileged(new PrivilegedExceptionAction&lt;Void&gt;() &#123; public Void run() throws RemoteException &#123; LiveRef var1x = new LiveRef(RegistryImpl.id, var1); RegistryImpl.this.setup(new UnicastServerRef(var1x, (var0) -&gt; &#123; return RegistryImpl.registryFilter(var0); &#125;)); return null; &#125; &#125;, (AccessControlContext)null, new SocketPermission("localhost:" + var1, "listen,accept")); &#125; catch (PrivilegedActionException var3) &#123; throw (RemoteException)var3.getException(); &#125; &#125; else &#123; LiveRef var2 = new LiveRef(id, var1); this.setup(new UnicastServerRef(var2, RegistryImpl::registryFilter)); &#125; &#125; 123RegistryImpl.this.setup(new UnicastServerRef(var1x, (var0) -&gt; &#123; return RegistryImpl.registryFilter(var0);&#125;)); setUp()方法将指向正在初始化的RegistryImpl对象的远程引用ref（RemoteRef）赋值为传入的UnicastServerRef对象，这里涉及了向上转型。然后继续移交UnicastServerRef的exportObject()方法。 进入UnicastServerRef的exportObject()方法。可以看到，这里首先为传入的RegistryImpl创建一个代理，这个代理我们可以推断出就是后面服务于客户端的RegistryImpl的Stub对象。然后将UnicastServerRef的skel（skeleton）对象设置为当前RegistryImpl对象。最后用skeleton、stub、UnicastServerRef对象、id和一个boolean值构造了一个Target对象，也就是这个Target对象基本上包含了全部的信息。调用UnicastServerRef的ref（LiveRef）变量的exportObject()方法。 到上面为止，我们看到的都是一些变量的赋值和创建工作，还没有到连接层，这些引用对象将会被Stub和Skeleton对象使用。接下来就是连接层上的了。追溯LiveRef的exportObject()方法，很容易找到了TCPTransport的exportObject()方法。这个方法做的事情就是将上面构造的Target对象暴露出去。调用TCPTransport的listen()方法，listen()方法创建了一个ServerSocket，并且启动了一条线程等待客户端的请求。接着调用父类Transport的exportObject()将Target对象存放进ObjectTable中 2. 将服务实现绑定到服务端的RegistryImpl上，使得客户端只需与RegistryImpl_Stub交互总结 当客户端通过RMI注册表找到一个远程接口的时候，所得到的其实是远程接口的一个动态代理对象。 当客户端调用其中的方法的时候，方法的参数对象会在序列化之后，传输到服务器端 服务器端接收到之后，进行反序列化得到参数对象 并使用这些参数对象，在服务器端调用实际的方法 调用的返回值Java对象经过序列化之后，再发送回客户端 客户端再经过反序列化之后得到Java对象，返回给调用者 这中间的序列化过程对于使用者来说是透明的，由动态代理对象自动完成 Stub和Skeleton的作用 Stub对象做的事情是建立到服务端Skeleton对象的Socket连接。将客户端的方法调用转换为字符串标识传递给Skeleton对象。并且同步阻塞等待服务端返回结果 Skeleton对象做的事情是将服务实现传入构造参数，获取客户端通过socket传过来的方法调用字符串标识，将请求转发到具体的服务上面。获取结果之后返回给客户端。]]></content>
      <categories>
        <category>Java后端开发</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>rmi</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CaffienCache缓存机制]]></title>
    <url>%2F2019%2F08%2F28%2F2019-08-28-CaffienCache%E7%BC%93%E5%AD%98%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[前言:今日在做项目优化时第一次引入Caffien Cache,该缓存机制能有效提升用户访问获取数据效率,我们可以将几乎不太变更的数据存入本地缓存，并定时更新缓存,在高并发场景将会有效提升系统性能 介绍: Caffeine is a high performance, near optimal caching library based on Java 8. For more details, see our user’s guide and browse the API docs for the latest release. Github链接 一. 三种缓存机制1. 手动加载该例子的运行机制为 Spring容器启动,单例BseBusinessIndexManagerImpl Bean被创建 PostConstruct()方法被执行,initCache()方法创建第一个缓存,一个定时线程被启动,每隔60分钟initCache()方法都会被运行,我们从数据库中获取最新的多条数据,put方法会自动更新原来key值对应的数据,但是这也会造成一个问题,过期key对应的缓存数据将不会被删除 关于get方法获取缓存数据,它的机制为每次先从缓存里查询,缓存中如果没有就调用降级方法去二级缓存或数据库中查找,并存入本地缓存 get 方法是以阻塞方式执行调用，即使多个线程同时请求该值也只会调用一次Function方法。这样可以避免与其他线程的写入竞争，这也是为什么使用 get 优于 getIfPresent 的原因 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253@Slf4j@Servicepublic class BseBusinessIndexManagerImpl implements BseBusinessIndexManager &#123; private Cache&lt;String, BseBusinessIndexPO&gt; INDEX_CACHE = Caffeine.newBuilder() .maximumSize(2048) .recordStats() .build(); @Autowired private BseBusinessIndexMapper businessIndexMapper; @PostConstruct public void init() &#123; initCache(); Executors.newSingleThreadScheduledExecutor(new ThreadFactoryBuilder() .setNameFormat("business-index-cache-updater").build()) .scheduleWithFixedDelay(() -&gt; initCache(), 60, 60, TimeUnit.MINUTES); &#125; @Override public BseBusinessIndexPO getIndexById(Integer id) &#123; return businessIndexMapper.selectByIndexId(id); &#125; @Override public List&lt;BseBusinessIndexPO&gt; getValidIndexes(String tableName, boolean isRealTime, List&lt;String&gt; fieldNames) &#123; return businessIndexMapper.getValidIndexes(tableName, isRealTime ? 1 : 0, fieldNames); &#125; @Override public BseBusinessIndexPO getCachedIndex(String tableName, String fieldName) &#123; return INDEX_CACHE.get("table.field:" + tableName + ":" + fieldName, k -&gt; &#123; String key = k.substring("table.field:".length()); String t = key.split(":")[0]; String f = key.split(":")[1]; return businessIndexMapper.selectByTableAndField(t, f); &#125;); &#125; private void initCache() &#123; List&lt;BseBusinessIndexPO&gt; indexes = businessIndexMapper.selectAll(); if (indexes.size() &gt; 1001) &#123; log.error("Business index count &gt; 1000"); &#125; for (BseBusinessIndexPO index : indexes) &#123; INDEX_CACHE.put("table.field:" + index.getTableName() + ":" + index.getFieldName(), index); INDEX_CACHE.put("id:" + index.getId(), index); &#125; &#125;&#125; 2. 同步加载1234567891011121314LoadingCache&lt;String, Object&gt; loadingCache = Caffeine.newBuilder() .maximumSize(10_000) .expireAfterWrite(10, TimeUnit.MINUTES) .build(key -&gt; createExpensiveGraph(key));String key = "name1";// 采用同步方式去获取一个缓存和上面的手动方式是一个原理。在build Cache的时候会提供一个createExpensiveGraph函数。// 查询并在缺失的情况下使用同步的方式来构建一个缓存Object graph = loadingCache.get(key);// 获取组key的值返回一个MapList&lt;String&gt; keys = new ArrayList&lt;&gt;();keys.add(key);Map&lt;String, Object&gt; graphs = loadingCache.getAll(keys); 3. 异步加载123456789101112131415161718AsyncLoadingCache&lt;String, Object&gt; asyncLoadingCache = Caffeine.newBuilder() .maximumSize(10_000) .expireAfterWrite(10, TimeUnit.MINUTES) // Either: Build with a synchronous computation that is wrapped as asynchronous .buildAsync(key -&gt; createExpensiveGraph(key)); // Or: Build with a asynchronous computation that returns a future // .buildAsync((key, executor) -&gt; createExpensiveGraphAsync(key, executor)); String key = "name1";// 查询并在缺失的情况下使用异步的方式来构建缓存CompletableFuture&lt;Object&gt; graph = asyncLoadingCache.get(key);// 查询一组缓存并在缺失的情况下使用异步的方式来构建缓存List&lt;String&gt; keys = new ArrayList&lt;&gt;();keys.add(key);CompletableFuture&lt;Map&lt;String, Object&gt;&gt; graphs = asyncLoadingCache.getAll(keys);// 异步转同步loadingCache = asyncLoadingCache.synchronous(); 二. 值回收1. 基于大小回收12345LoadingCache&lt;String, DataObject&gt; cache = Caffeine.newBuilder() .maximumSize(1) .build(k -&gt; DataObject.get("Data for " + k));assertEquals(0, cache.estimatedSize()); 2. 基于时间回收未完待续… 访问后到期 123LoadingCache&lt;String, DataObject&gt; cache = Caffeine.newBuilder() .expireAfterAccess(5, TimeUnit.MINUTES) .build(k -&gt; DataObject.get("Data for " + k)); 写入后到期 12345cache = Caffeine.newBuilder() .expireAfterWrite(10, TimeUnit.SECONDS) .weakKeys() .weakValues() .build(k -&gt; DataObject.get("Data for " + k)); 自定义策略 三. 刷新123Caffeine.newBuilder() .refreshAfterWrite(1, TimeUnit.MINUTES) .build(k -&gt; DataObject.get("Data for " + k)); 这里我们要明白 expireAfter 和 refreshAfter 之间的区别。当请求过期条目时，执行将发生阻塞，直到 build Function 计算出新值为止。 但是，如果条目可以刷新，则缓存将返回一个旧值，并异步重新加载该值 1未完待续...关于Cache在SpringBoot上的集成]]></content>
      <categories>
        <category>Java后端开发</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>spring</tag>
        <tag>cache</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring MVC原理简介]]></title>
    <url>%2F2019%2F08%2F27%2F2019-08-26-SpringMVC%E5%8E%9F%E7%90%86%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[一次请求处理过程 请求： 将请求提交给服务器，大多数框架都有一个调度程序（以servlet的形式）用来处理请求 调用： DispatchServlet根据HTTP请求信息和Web应用程序配置将请求分配给适当的控制器 服务调用：控制器与服务层交互 填充模型：控制器使用从服务层获得的信息填充模型 创建视图：根据模型创建视图 响应 ajax调用 请求: 准备XMLHttpRequest并提交给服务器。 调度程序将请求分派给适当的控制器 响应: 控制器与服务层交互，相应数据将被格式化并发送到浏览器，此时并不涉及任何试图。浏览器接收数据并对现有视图进行部分更新]]></content>
      <categories>
        <category>Java后端开发</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>spring</tag>
        <tag>mvc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Cloud]]></title>
    <url>%2F2019%2F08%2F27%2F2019-08-27-SpringCloud%2F</url>
    <content type="text"><![CDATA[前言: Spring Cloud是Netflix公司开发的一款微服务框架,今天在公司迁移项目时折腾SpringCloud配置折腾了一下午，所以在这里总结一下Spring Cloud的生产者和消费者是如何链条的以及SpringBoot相关配置 原理图 实际使用1.]]></content>
      <categories>
        <category>Java后端开发</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>spring</tag>
        <tag>springcloud</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MyBatis在SpringBoot上的集成运用]]></title>
    <url>%2F2019%2F08%2F25%2F2019-08-25-Mybatis%2F</url>
    <content type="text"><![CDATA[前言: 最近做公司的项目时,用到的访问数据库CRUD框架为mybatis,实际使用起来经常踩坑，于是总结一下基本使用方法和踩过的坑 基本介绍：mybatis是一款基于JAVA JDBC API [^1] 的持久层框架,它支持将Java POJOs 映射成数据库中对应的记录 [^1]:Java Database connection(java数据库连接) 一. 添加依赖12345&lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis&lt;/artifactId&gt; &lt;version&gt;x.x.x&lt;/version&gt;&lt;/dependency&gt; 二. 配置SqlSessionFactory的两种方法基本定义 sqlSession Factory: 创建sqlSession的工厂,SqlSessionFactory是MyBatis的关键对象,它是个单个数据库映射关系经过编译后的内存镜像.SqlSessionFactory对象的实例可以通过SqlSessionFactoryBuilder对象类获得,而SqlSessionFactoryBuilder则可以从XML配置文件或一个预先定制的Configuration的实例构建出SqlSessionFactory的实例.每一个MyBatis的应用程序都以一个SqlSessionFactory对象的实例为核心.同时SqlSessionFactory也是线程安全的,SqlSessionFactory一旦被创建,应该在应用执行期间都存在.在应用运行期间不要重复创建多次,建议使用单例模式 sql session sqlSession: 应用程序和数据库之间的一个单线程对象,类似于JDBC的connection,线程不安全,是具体执行sql语句的实例 xml基于xml配置定义数据库源和mapper路径 123456789101112131415161718192021222324252627282930313233&lt;?xml version="1.0" encoding="UTF-8" ?&gt;&lt;!DOCTYPE configuration PUBLIC "-//mybatis.org//DTD Config 3.0//EN" "http://mybatis.org/dtd/mybatis-3-config.dtd"&gt;&lt;configuration&gt;&lt;!-- 引入外部资源文件 --&gt;&lt;properties resource="jdbc.properties"&gt;&lt;/properties&gt;&lt;!-- 设置驼峰匹配 --&gt;&lt;settings&gt; &lt;setting name="mapUnderscoreToCamelCase" value="true"/&gt;&lt;/settings&gt;&lt;!-- 设置包扫描(别名) --&gt;&lt;typeAliases&gt; &lt;package name="cn.itcast.pojo"/&gt;&lt;/typeAliases&gt;&lt;!-- 配置环境：可以配置多个环境，default：配置某一个环境的唯一标识，表示默认使用哪个环境 --&gt; &lt;environments default="development"&gt; &lt;environment id="development"&gt; &lt;transactionManager type="JDBC"/&gt; &lt;dataSource type="POOLED"&gt; &lt;!-- 配置连接信息 --&gt; &lt;property name="driver" value="$&#123;jdbc.driverClass&#125;"/&gt; &lt;property name="url" value="$&#123;jdbc.url&#125;"/&gt; &lt;property name="username" value="$&#123;jdbc.username&#125;"/&gt; &lt;property name="password" value="$&#123;jdbc.password&#125;"/&gt; &lt;/dataSource&gt; &lt;/environment&gt; &lt;/environments&gt; &lt;!-- 配置映射文件：用来配置sql语句和结果集类型等 --&gt; &lt;mappers&gt; &lt;mapper resource="UserMapper.xml"/&gt; &lt;/mappers&gt;&lt;/configuration&gt; Configuration123456DataSource dataSource = BlogDataSourceFactory.getBlogDataSource();TransactionFactory transactionFactory = new JdbcTransactionFactory();Environment environment = new Environment("development", transactionFactory, dataSource);Configuration configuration = new Configuration(environment);configuration.addMapper(BlogMapper.class);SqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(configuration); sqlSession1234try (SqlSession session = sqlSessionFactory.openSession()) &#123; BlogMapper mapper = session.getMapper(BlogMapper.class); Blog blog = mapper.selectBlog(101);&#125; 定义SQL语句的两种方法 mapper.xml 123456789&lt;?xml version="1.0" encoding="UTF-8" ?&gt;&lt;!DOCTYPE mapper PUBLIC "-//mybatis.org//DTD Mapper 3.0//EN" "http://mybatis.org/dtd/mybatis-3-mapper.dtd"&gt;&lt;mapper namespace="org.mybatis.example.BlogMapper"&gt; &lt;select id="selectBlog" resultType="Blog"&gt; select * from Blog where id = #&#123;id&#125; &lt;/select&gt;&lt;/mapper&gt; 注解 12345package org.mybatis.example;public interface BlogMapper &#123; @Select("SELECT * FROM blog WHERE id = #&#123;id&#125;") Blog selectBlog(int id);&#125; 三. 与SpringBoot的集成1.配置application.yml文件/application.properties 这一步操作的工作与mybatis.xml的作用类似，定义数据库源和需要扫描的相关mapper的路径 application.properties:1234567891011121314151617181920212223# mybatismybatis.mapperLocations=classpath:mapper/*/*.xmlmybatis.typeAliasesPackage=com.xxx.xxx.xxx.webapi.daomybatis.configuration.map-underscore-to-camel-case=truemybatis.mapper3.identity=MYSQLmybatis.mapper3.notEmpty=true#表明POJO中的驼峰自动转换成数据库中的下划线mybatis.mapper3.style=camelhumpmybatis.mapper3.mapperLocations=classpath:mapper/*.xml#自动扫描自定义POJO的路径mybatis.mapper3.typeAliasesPackage=com.xxx.xxx.xxx.webapi.domainmybatis.mapper3.pageUsable=truemybatis.mapper3.pageDialect=mysqlmybatis.mapper3.pageReasonable=truemybatis.mapper3.pageSupportMethodsArguments=truemybatis.mapper3.pageReturnPageInfo=checkmybatis.mapper3.pageParams=count=countSql# dsspring.secure.ds.default.database=beespring.secure.ds.default.url=jdbc:mysql://001.bi.mysql.test.xxx.info:3308/beespring.secure.ds.default.username=usernamespring.secure.ds.default.password=pwd application.yml1234567891011121314151617181920212223242526272829303132333435server: port: 8080spring: datasource: name: test url: jdbc:mysql://127.0.0.1:3306/depot username: root password: root # 使用druid数据源 type: com.alibaba.druid.pool.DruidDataSource driver-class-name: com.mysql.jdbc.Driver filters: stat maxActive: 20 initialSize: 1 maxWait: 60000 minIdle: 1 timeBetweenEvictionRunsMillis: 60000 minEvictableIdleTimeMillis: 300000 validationQuery: select 'x' testWhileIdle: true testOnBorrow: false testOnReturn: false poolPreparedStatements: true maxOpenPreparedStatements: 20mybatis: mapper-locations: classpath:mapping/*.xml type-aliases-package: com.winter.model#pagehelper分页插件pagehelper: helperDialect: mysql reasonable: true supportMethodsArguments: true params: count=countSql 2.mapper.xml(这一步可省略)3.定义Mapper接口 123456789101112package com.wacai.stanlee.bullseye.webapi.dao;import com.wacai.stanlee.bullseye.webapi.domain.BseAppSidePO;import org.apache.ibatis.annotations.Param;import org.apache.ibatis.annotations.Select;import tk.mybatis.mapper.common.Mapper;import tk.mybatis.mapper.common.MySqlMapper;public interface BseAppSideMapper extends MySqlMapper&lt;BseAppSidePO&gt;, Mapper&lt;BseAppSidePO&gt; &#123; @Select("select * from bee_app_side where is_deleted=1 and bee_app_main_id=#&#123;beeAppMainId&#125; limit 1") BseAppSidePO selectByAppId(@Param("beeAppMainId") Integer beeAppMainId);&#125; 4.在服务启动的主方法类上添加org.mybatis.spring.annotation.MapperScan配置，作用：在服务启动的时候全局配置系统对mapper接口扫描。 1234567891011121314package com.winter;import org.mybatis.spring.annotation.MapperScan;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;@SpringBootApplication@MapperScan("com.winter.mapper")//将项目中对应的mapper类的路径加进来就可以了public class SpringbootMybatisDemoApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(SpringbootMybatisDemoApplication.class, args); &#125;&#125; 四. 实际使用时的注意事项1.mybatis对于 in 数组 的支持非常不好，建议用find_in_set(array,num)代替2.mybatis @Select(“select * from …”)若其中某个字段为字符串 “1,2,3”,而我想将其转换成list,默认的映射无法做到，这时可以自定义一个typeHandler继承BaseTypeHandler,并在mybatis的配置文件中加入 12345678910111213141516171819202122232425262728293031323334353637public class IntArrayTypeHandler extends BaseTypeHandler&lt;int[]&gt; &#123; @Override public void setNonNullParameter(PreparedStatement ps, int i, int[] parameter, JdbcType jdbcType) throws SQLException &#123; List&lt;String&gt; list = new ArrayList&lt;&gt;(); for (int item : parameter) &#123; list.add(String.valueOf(item)); &#125; ps.setString(i, String.join(",", list)); &#125; @Override public int[] getNullableResult(ResultSet rs, String columnName) throws SQLException &#123; String str = rs.getString(columnName); if (rs.wasNull()) return null; return Arrays.stream(str.split(",")).mapToInt(Integer::valueOf).toArray(); &#125; @Override public int[] getNullableResult(ResultSet rs, int columnIndex) throws SQLException &#123; String str = rs.getString(columnIndex); if (rs.wasNull()) return null; return Arrays.stream(str.split(",")).mapToInt(Integer::valueOf).toArray(); &#125; @Override public int[] getNullableResult(CallableStatement cs, int columnIndex) throws SQLException &#123; String str = cs.getString(columnIndex); if (cs.wasNull()) return null; return Arrays.stream(str.split(",")).mapToInt(Integer::valueOf).toArray(); &#125;&#125; 123&lt;typeHandlers&gt; &lt;typeHandler handler="com.jarhf.mybatis.handler.IntArrayTypeHandler"/&gt;&lt;/typeHandlers&gt; 3.@Result搭配Ibatis的@Select注解使用 最近在使用mybatis遇到的一个坑,javax.persistence的@Column[^2]在原生ibatis的@Select注解上不起作用,@Column注解只有搭配mybatis预定义方法和才起作用 @Select必须搭配@Result注解来自定义类与表字段的映射[^2]:Specifies the mapped column for a persistent property or field. If no Column annotation is specified, the default values apply. 12345678@Select("select * from bee_deliverys where find_in_set(#&#123;beeAppId&#125;,bee_app_id) and channel_id=#&#123;channelId&#125; and is_deleted=0") @Results(&#123; @Result(column = "account_name", property = "loginName"), @Result(property = "creator", column = "authors"), @Result(property = "blineId", column = "bee_bline_id"), @Result(property = "appId", column = "bee_app_id") &#125;) List&lt;PutinAccount&gt; selectByAppIdAndChannel(@Param("channelId") Integer channelId, @Param("beeAppId") Integer beeAppId); 或者也可以使用xml配置文件中的resultMap解决这一问题 1234567891011&lt;resultMap type="com.test" id="testResultMap"&gt; &lt;!-- property对应实体类的属性名称，column为数据库结果集的列的名称 --&gt; &lt;id property="id" column="id" /&gt; &lt;result property="parentId" column="parentId"/&gt; &lt;result property="name" column="name"/&gt; &lt;result property="enName" column="enName"/&gt;&lt;/resultMap&gt;&lt;select id="selectList" resultMap="testResultMap"&gt; select * from test1 &lt;/select&gt; 五. 小结对于框架背后原理的理解可以提升使用效率，本篇只是一个开端，主要总结了一下mybatis基础使用方法，没有涉及到的内容还有很多，例如mybatis的事物管理机制,mybatis连接数据库背后的原理分析,之后再深入学习一下，再做更新:)]]></content>
      <categories>
        <category>Java后端开发</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>myBatis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java NIO底层原理]]></title>
    <url>%2F2019%2F08%2F24%2F2019-08-18-NIO%2F</url>
    <content type="text"><![CDATA[前言：最近在看dubbo框架底层依赖协议时注意到了java NIO，由于操作系统和I/O相关的底层原理一直没有好好研究过，感到一头雾水，于是决定写篇博客学习一下 Java IO读写原理 read系统调用 是把数据从内核缓冲区复制到进程缓冲区 write系统调用 把数据从进程缓冲区复制到内核缓冲区 kernal 负责数据在内核缓冲区和磁盘之间的交换 java读写流程 客户端请求 Linux通过网卡，读取客户断的请求数据，将数据读取到内核缓冲区 获取请求数据 服务器从内核缓冲区读取数据到Java进程缓冲区。 服务器端业务处理 Java服务端在自己的用户空间中，处理客户端的请求。 服务器端返回数据 Java服务端已构建好的响应，从用户缓冲区写入系统缓冲区。 发送给客户端 Linux内核通过网络 I/O ，将内核缓冲区中的数据，写入网卡，网卡通过底层的通讯协议，会将数据发送给目标客户端。 同步阻塞用户线程在系统调用的整个IO过程中都是阻塞的（1.等待数据到达内核缓冲区 2.将数据拷贝） 同步非阻塞用户线程在系统调用的 等待数据到达内核缓冲区(kernal buffer) 非阻塞 在内核缓冲区没有数据的情况下，系统调用会立即返回，返回一个调用失败的信息 用户线程需要不断的发起 I/O调用 同时可以做别的处理 拷贝数据到用户缓冲区(user buffer) 阻塞 多路复用模型 进行select/epoll系统调用，查询可以读的连接。kernel会查询所有select的可查询socket列表，当任何一个socket中的数据准备好了，select就会返回。当用户进程调用了select，那么整个线程会被block（阻塞掉）。 用户线程获得了目标连接后，发起read系统调用，用户线程阻塞。内核开始复制数据。它就会将数据从kernel内核缓冲区，拷贝到用户缓冲区（用户内存），然后kernel返回结果。 用户线程才解除block的状态，用户线程终于真正读取到数据，继续执行。 多路复用的两种模式 优点: 用select/epoll的优势在于，它可以同时处理成千上万个连接（connection）。与一条线程维护一个连接相比，I/O多路复用技术的最大优势是：系统不必创建线程，也不必维护这些线程，从而大大减小了系统的开销 异步非阻塞 AIO的基本流程是：用户线程通过系统调用，告知kernel内核启动某个IO操作，用户线程返回。kernel内核在整个IO操作（包括数据准备、数据复制）完成后，通知用户程序，用户执行后续的业务操作。]]></content>
      <categories>
        <category>Java后端开发</category>
      </categories>
      <tags>
        <tag>IO</tag>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[微服务:SpringCloudVsDubbo]]></title>
    <url>%2F2019%2F08%2F17%2F2019-08-17-%E5%BE%AE%E6%9C%8D%E5%8A%A1-SpringCloudVsDubbo%2F</url>
    <content type="text"><![CDATA[基本介绍 微服务架构是互联网很热门的话题，是互联网技术发展的必然结果。它提倡将单一应用程序划分成一组小的服务，服务之间互相协调、互相配合，为用户提供最终价值。虽然微服务架构没有公认的技术标准和规范或者草案，但业界已经有一些很有影响力的开源微服务架构框架提供了微服务的关键思路，例如Dubbo和Spring Cloud。各大互联网公司也有自研的微服务框架，但其模式都于这二者相差不大。 微服务主要的优势如下： 降低复杂度将原来偶合在一起的复杂业务拆分为单个服务，规避了原本复杂度无止境的积累。每一个微服务专注于单一功能，并通过定义良好的接口清晰表述服务边界。每个服务开发者只专注服务本身，通过使用缓存、DAL等各种技术手段来提升系统的性能，而对于消费方来说完全透明。 可独立部署由于微服务具备独立的运行进程，所以每个微服务可以独立部署。当业务迭代时只需要发布相关服务的迭代即可，降低了测试的工作量同时也降低了服务发布的风险。 容错在微服务架构下，当某一组件发生故障时，故障会被隔离在单个服务中。 通过限流、熔断等方式降低错误导致的危害，保障核心业务正常运行。 扩展单块架构应用也可以实现横向扩展，就是将整个应用完整的复制到不同的节点。当应用的不同组件在扩展需求上存在差异时，微服务架构便体现出其灵活性，因为每个服务可以根据实际需求独立进行扩展。 微服务思维导图 总体架构 Dubbo Spring Cloud核心要素 核心要素 Dubbo Spring Cloud 服务注册中心 Zookeeper Reddis Spring Cloud Netflix Eureka 服务调用方式 RPC Rest API 服务网关 无 Spring Cloud Netflix Zuul 断路器 / Spring Cloud Netflix Hystrix 分布式配置 / Spring Cloud Config 分布式追踪系统 / Spring Cloud Sleuth 消息总线 / Spring Cloud Bus 数据流 / Spring Cloud Stream 基于Reddis,Rabbit,Kafka实现的消息微服务 批量任务 / Spring Cloud Task 依赖协议Spring Cloud 使用HTTP协议的REST API Dubbo dubbo：Dubbo缺省协议采用单一长连接和NIO异步通讯，适合于小数据量大并发的服务调用，以及服务消费者机器数远大于服务提供者机器数的情况 rmi：RMI协议采用JDK标准的java.rmi.实现，采用阻塞式短连接和JDK标准序列化方式 Hessian:Hessian协议用于集成Hessian的服务，Hessian底层采用Http通讯，采用Servlet暴露服务，Dubbo缺省内嵌Jetty作为服务器实现 http:采用Spring的HttpInvoker实现 Spring Cloud 工作流程 created: 2019/08/17updated: 2019/08/17未完待续]]></content>
      <categories>
        <category>Java后端开发</category>
      </categories>
      <tags>
        <tag>micro-service</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[实时流计算之CEP实践]]></title>
    <url>%2F2019%2F07%2F01%2F2019-07-01-%E5%AE%9E%E6%97%B6%E6%B5%81cep%2F</url>
    <content type="text"><![CDATA[流数据 持续到达 无限增长 到达次序独立 流计算 使用flink处理流数据 watermark 衡量时间进制的标准 时间窗口：10s 容错机制 CEP介绍基于flink任务ex:实时营销：用户第一次激活几小时内触发某个行为def： complex Event Process 对象：append-only time-ordered sequence of events 机制： continuous CEP与SQL 解决什么问题 过滤掉不满足条件的时间， 找到符合filter定义的事件 eg: 从还款事件中筛选出逾期还款事件 找到符合pattern的事件 eg:贷款申请审批通过一小时后未提款 事件流join数据表 筛选出贷款事件中的女性用户 窗口聚合 CEP平台（ICEYE） 事件配置 规则定义 结果消费 应用 Trident实时营销 后裔实时报表 特点]]></content>
      <categories>
        <category>Java后端开发</category>
      </categories>
      <tags>
        <tag>cep</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spirng-boot-controller]]></title>
    <url>%2F2019%2F06%2F28%2F2019-06-28-spirng-boot%20controller%2F</url>
    <content type="text"><![CDATA[spring 中 Beanhttp://www.voidcn.com/article/p-peghajxt-bpb.htmlhttps://www.awaimai.com/2596.htmlhttps://juejin.im/post/5b1b841de51d4506d47df108org.springframework.web.bind.annotation reference: https://www.baeldung.com/spring-mvc-annotations @Controller: used with @requestMapping 1234567891011@Controller@RequestMapping("books")public class SimpleBookController &#123; @GetMapping("/&#123;id&#125;", produces = "application/json") public @ResponseBody Book getBook(@PathVariable int id) &#123; return findBookById(id); &#125; private Book findBookById(int id) &#123; // ... &#125;&#125; @RestController==@Controller@ResponseBody: render the result string directly back to the caller serialize the return objects into http response 1234567891011@RestController@RequestMapping("books-rest")public class SimpleBookRestController &#123; @GetMapping("/&#123;id&#125;", produces = "application/json") public Book getBook(@PathVariable int id) &#123; return findBookById(id); &#125; private Book findBookById(int id) &#123; // ... &#125;&#125; @RequestMapping: map specific http requests to specific method 1234567@Controllerclass VehicleController &#123;@RequestMapping(value = "/vehicles/home", method = RequestMethod.GET)String home() &#123; return "home";&#125;&#125; path method params headers consumes produces(media type that can be produced) @Getmapping @Postmapping @PutMapping @DeleteMapping @PatchMappinge @ResponseBody: 直接将返回的对象输出到客户端，如果是字符串，则直接返回；否则spring-boot默认使用jackson serialize 成 json字符串 @RequestBody: Controller方法带有@RequestBody注解的参数，意味着request 内容是json, spring-boot默认使用jackson deserialize maps the body of the Http request to an object automatically 1234@PostMapping("/save")void saveVehicle(@RequestBody Vehicle vehicle) &#123; // ...&#125; @ModelAttribute 123456789101112131415161718192021@Controller@ControllerAdvicepublic class EmployeeController &#123; private Map&lt;Long, Employee&gt; employeeMap = new HashMap&lt;&gt;(); @RequestMapping(value = "/addEmployee", method = RequestMethod.POST) public String submit( @ModelAttribute("employee") Employee employee, BindingResult result, ModelMap model) &#123; if (result.hasErrors()) &#123; return "error"; &#125; model.addAttribute("name", employee.getName()); model.addAttribute("id", employee.getId()); employeeMap.put(employee.getId(), employee); return "employeeView"; &#125; @ModelAttribute public void addAttributes(Model model) &#123; model.addAttribute("msg", "Welcome to the Netherlands!"); &#125;&#125; @EnableAutoConfiguration @RequestParam 1234@RequestMapping("/buy")Car buyCar(@RequestParam(defaultValue = "5") int seatCount) &#123; // ...&#125; @PathVariable 1234@RequestMapping("/&#123;id&#125;")Vehicle getVehicle(@PathVariable("id") long id) &#123; // ...&#125; @CookieValue @RequestHeader @ResponseBody:treats the return of the method as the response itself Other Annotations @Autowired reference: https://www.jianshu.com/p/9062a92fbf9a https://blog.csdn.net/HEYUTAO007/article/details/5981555 @Data]]></content>
      <categories>
        <category>Java后端开发</category>
      </categories>
      <tags>
        <tag>spring-boot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Framework]]></title>
    <url>%2F2019%2F06%2F27%2F2019-06-27-SpringFramework%2F</url>
    <content type="text"><![CDATA[Ioc Container(DI)reference:https://docs.spring.io/spring-framework/docs/current/spring-framework-reference/core.html def:objects define their dependencies only through constructor arguments org.springframework.beans org.springframework.context A bean is an object that is instantiated, assembled, and otherwise managed by a Spring IoC container. Container Overviewconfiguring metadata xml based this bean definition corresponds to the actual objects service layer data access objects annotation based java-based instantiating a Container1ApplicationContext context = new ClassPathXmlApplicationContext("services.xml", "daos.xml"); service.xml 1234567891011121314151617&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.springframework.org/schema/beans https://www.springframework.org/schema/beans/spring-beans.xsd"&gt; &lt;!-- services --&gt; &lt;bean id="petStore" class="org.springframework.samples.jpetstore.services.PetStoreServiceImpl"&gt; &lt;property name="accountDao" ref="accountDao"/&gt; &lt;property name="itemDao" ref="itemDao"/&gt; &lt;!-- additional collaborators and configuration for this bean go here --&gt; &lt;/bean&gt; &lt;!-- more bean definitions for services go here --&gt;&lt;/beans&gt; Dependency constructor-based Dependency setter-based Annotation-based@Required for setter method | @Autowired for constructor 1234567891011public class MovieRecommender &#123; private final CustomerPreferenceDao customerPreferenceDao; @Autowired public MovieRecommender(CustomerPreferenceDao customerPreferenceDao) &#123; this.customerPreferenceDao = customerPreferenceDao; &#125; // ...&#125; As of Spring Framework 4.3, an @Autowired annotation on such a constructor is no longer necessary if the target bean defines only one constructor to begin with. However, if several constructors are available, at least one must be annotated to teach the container which one to use. traditional constructor 1234567891011public class MovieRecommender &#123; private final CustomerPreferenceDao customerPreferenceDao; @Autowired public MovieRecommender(CustomerPreferenceDao customerPreferenceDao) &#123; this.customerPreferenceDao = customerPreferenceDao; &#125; // ...&#125; setter 1234567891011public class SimpleMovieLister &#123; private MovieFinder movieFinder; @Autowired public void setMovieFinder(MovieFinder movieFinder) &#123; this.movieFinder = movieFinder; &#125; // ...&#125; arbitrary names and multiple arguments 123456789101112131415public class MovieRecommender &#123; private MovieCatalog movieCatalog; private CustomerPreferenceDao customerPreferenceDao; @Autowired public void prepare(MovieCatalog movieCatalog, CustomerPreferenceDao customerPreferenceDao) &#123; this.movieCatalog = movieCatalog; this.customerPreferenceDao = customerPreferenceDao; &#125; // ...&#125; fileds 1234567891011121314public class MovieRecommender &#123; private final CustomerPreferenceDao customerPreferenceDao; @Autowired private MovieCatalog movieCatalog; @Autowired public MovieRecommender(CustomerPreferenceDao customerPreferenceDao) &#123; this.customerPreferenceDao = customerPreferenceDao; &#125; // ...&#125; arrays and collections 1234567891011public class MovieRecommender &#123; private Set&lt;MovieCatalog&gt; movieCatalogs; @Autowired public void setMovieCatalogs(Set&lt;MovieCatalog&gt; movieCatalogs) &#123; this.movieCatalogs = movieCatalogs; &#125; // ...&#125; map 1234567891011public class MovieRecommender &#123; private Map&lt;String, MovieCatalog&gt; movieCatalogs; @Autowired public void setMovieCatalogs(Map&lt;String, MovieCatalog&gt; movieCatalogs) &#123; this.movieCatalogs = movieCatalogs; &#125; // ...&#125; autowiring fails when no matching candiate beans are available AOPAspect Oriented Programming]]></content>
      <categories>
        <category>Java后端开发</category>
      </categories>
      <tags>
        <tag>spring-boot</tag>
        <tag>coding</tag>
      </tags>
  </entry>
</search>
